{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAxwhwJVQztT"
   },
   "source": [
    "# AUTOFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 132500,
     "status": "ok",
     "timestamp": 1744697340087,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "XUSdk9HXQ4bz",
    "outputId": "94cf82db-6caf-4bc1-c28e-89db8a39816a"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, TimeDistributed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "\n",
    "ruta_csv = \"/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\"\n",
    "df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "df[\"con_fecha\"] = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "df['fecha_embarque'] = pd.to_datetime(df['fecha_embarque'], errors='coerce')\n",
    "\n",
    "columnas_a_mantener = [\n",
    "    'fecha_embarque','tipo_agrupacion',\n",
    "    'dia_del_anio_sin', 'dia_del_anio_cos',\n",
    "    'dia_embarque_sin', 'dia_embarque_cos',\n",
    "    'hora_embarque_sin', 'hora_embarque_cos',\n",
    "    'is_weekend',\n",
    "    'mes_embarque_sin', 'mes_embarque_cos',\n",
    "    'week_of_year_sin', 'week_of_year_cos',\n",
    "    'season_1', 'season_2', 'season_3', 'season_4',\n",
    "    'is_festivo_nacional', 'is_festivo_local',\n",
    "    'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "    'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "    'is_mawlid_nabi',\n",
    "    'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "    'is_friday', 'is_saturday', 'is_sunday',\n",
    "    'weekday_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "df = df[columnas_a_mantener]\n",
    "df_pasajeros = df[df[\"tipo_agrupacion\"] == 1].copy()\n",
    "df_vehiculos = df[df[\"tipo_agrupacion\"] == 0].copy()\n",
    "\n",
    "def procesar_dataset_completo(df_original, df_filtrado, nombre_target):\n",
    "\n",
    "    rango_fechas = pd.date_range(\n",
    "        start=df_original['fecha_embarque'].min(),\n",
    "        end=df_original['fecha_embarque'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    df_daily = (\n",
    "        df_filtrado.groupby('fecha_embarque')\n",
    "        .size()\n",
    "        .reindex(rango_fechas, fill_value=0)\n",
    "        .reset_index(name=nombre_target)\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    df_temp = (\n",
    "        df_original.drop(columns=['tipo_agrupacion'])\n",
    "        .drop_duplicates('fecha_embarque')\n",
    "        .set_index('fecha_embarque')\n",
    "        .reindex(rango_fechas)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    cols_categoricas = [\n",
    "        'is_weekend', 'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi', 'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday'\n",
    "    ]\n",
    "\n",
    "    for col in cols_categoricas:\n",
    "        df_temp[col] = df_temp[col].fillna(method='ffill').fillna(0)\n",
    "\n",
    "    df_temp['dia_del_anio'] = df_temp['fecha_embarque'].dt.dayofyear\n",
    "    df_temp['dia_del_anio_sin'] = np.sin(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "    df_temp['dia_del_anio_cos'] = np.cos(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "\n",
    "    df_temp['dia_embarque'] = df_temp['fecha_embarque'].dt.day\n",
    "    df_temp['dia_embarque_sin'] = np.sin(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "    df_temp['dia_embarque_cos'] = np.cos(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "\n",
    "    df_temp['mes_embarque'] = df_temp['fecha_embarque'].dt.month\n",
    "    df_temp['mes_embarque_sin'] = np.sin(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "    df_temp['mes_embarque_cos'] = np.cos(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "\n",
    "    df_temp['week_of_year'] = df_temp['fecha_embarque'].dt.isocalendar().week\n",
    "    df_temp['week_of_year_sin'] = np.sin(2 * np.pi * df_temp['week_of_year']/52)\n",
    "    df_temp['week_of_year_cos'] = np.cos(2 * np.pi * df_temp['week_of_year']/52)\n",
    "\n",
    "    if df_temp['weekday_sin'].isna().any() or df_temp['weekday_cos'].isna().any():\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday  \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp = df_temp.drop(columns=['weekday'])\n",
    "\n",
    "    df_temp = df_temp.drop(columns=['dia_del_anio', 'dia_embarque', 'mes_embarque', 'week_of_year'])\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_temp,\n",
    "        df_daily,\n",
    "        on='fecha_embarque',\n",
    "        how='left'\n",
    "    ).fillna({nombre_target: 0})\n",
    "\n",
    "    df_final = df_final.sort_values('fecha_embarque')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f'{nombre_target}_norm'] = scaler.fit_transform(df_final[[nombre_target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    return df_final, scaler\n",
    "\n",
    "df_pasajeros_daily, scaler_pasajeros = procesar_dataset_completo(df, df_pasajeros, 'total_pasajeros')\n",
    "df_vehiculos_daily, scaler_vehiculos = procesar_dataset_completo(df, df_vehiculos, 'total_vehiculos')\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7):\n",
    "    features = df.columns.difference(['fecha_embarque', target_col, f'{target_col}_norm'])\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(df)):\n",
    "        X_seq = df.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X.append(X_seq)\n",
    "        y.append(df.iloc[i][f'{target_col}_norm'])\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "LOOKBACK = 7\n",
    "X_pasajeros, y_pasajeros = crear_secuencias(df_pasajeros_daily, 'total_pasajeros', LOOKBACK)\n",
    "X_vehiculos, y_vehiculos = crear_secuencias(df_vehiculos_daily, 'total_vehiculos', LOOKBACK)\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    train_end = int(len(X) * ratios[0])\n",
    "    val_end = train_end + int(len(X) * ratios[1])\n",
    "    return (X[:train_end], y[:train_end]), (X[train_end:val_end], y[train_end:val_end]), (X[val_end:], y[val_end:])\n",
    "\n",
    "(train_p, val_p, test_p) = split_temporal(X_pasajeros, y_pasajeros)\n",
    "(train_v, val_v, test_v) = split_temporal(X_vehiculos, y_vehiculos)\n",
    "\n",
    "train_end_p = int(len(X_pasajeros) * 0.7)\n",
    "val_end_p = train_end_p + int(len(X_pasajeros) * 0.15)\n",
    "test_dates = df_pasajeros_daily['fecha_embarque'].iloc[-len(test_p[0]):].values\n",
    "\n",
    "print(\"\\n=== Preparacion de datos completada ===\")\n",
    "print(f\"Train set: {train_p[0].shape}\")\n",
    "print(f\"Validation set: {val_p[0].shape}\")\n",
    "print(f\"Test set: {test_p[0].shape}\")\n",
    "\n",
    "class SeriesDecomposition(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, kernel_size=25):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_trend = Conv1D(filters=1, kernel_size=self.kernel_size,\n",
    "                                 padding='same', strides=1, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        trend = self.conv_trend(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class AutoCorrelationBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        attn_output = self.mha(x, x, x)\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        return self.ln(x + attn_output)\n",
    "\n",
    "class AutoformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, kernel_size=25, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.decomp = SeriesDecomposition(kernel_size=kernel_size)\n",
    "        self.auto_corr = AutoCorrelationBlock(d_model, num_heads, dropout=dropout)\n",
    "\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        seasonal_part, trend_part = self.decomp(x) \n",
    "        seasonal_part = self.auto_corr(seasonal_part, training=training)\n",
    "        x_out = seasonal_part + trend_part\n",
    "        x_out2 = self.ff(x_out)\n",
    "        x_out2 = self.dropout(x_out2, training=training)\n",
    "        x_out = self.ln(x_out + x_out2)\n",
    "        return x_out\n",
    "\n",
    "class AutoformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, kernel_size=25, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            AutoformerEncoderLayer(d_model, num_heads, d_ff, kernel_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        position = np.arange(0, sequence_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pos_encoding = np.zeros((sequence_length, d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding\n",
    "\n",
    "def crear_modelo_autoformer(\n",
    "    input_shape,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=128,\n",
    "    num_encoder_layers=2,\n",
    "    kernel_size=25,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)  \n",
    "\n",
    "    x = TimeDistributed(Dense(d_model))(inputs)\n",
    "\n",
    "    x = PositionalEncoding(input_shape[0], d_model)(x)\n",
    "\n",
    "    x = AutoformerEncoder(\n",
    "        num_layers=num_encoder_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout_rate\n",
    "    )(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def crear_callbacks(model_name):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{model_name}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    return [early_stopping, checkpoint, reduce_lr]\n",
    "\n",
    "print(\"\\n=== Creando y entrenando el modelo Autoformer para pasajeros ===\")\n",
    "\n",
    "input_shape = (train_p[0].shape[1], train_p[0].shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "modelo_autoformer = crear_modelo_autoformer(\n",
    "    input_shape=input_shape,\n",
    "    d_model=64,         \n",
    "    num_heads=4,        \n",
    "    d_ff=128,           \n",
    "    num_encoder_layers=2,\n",
    "    kernel_size=25,    \n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "modelo_autoformer.summary()\n",
    "\n",
    "callbacks = crear_callbacks(\"autoformer_model_pasajeros\")\n",
    "\n",
    "history = modelo_autoformer.fit(\n",
    "    train_p[0], train_p[1],\n",
    "    validation_data=(val_p[0], val_p[1]),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluando el modelo Autoformer pasajeros ===\")\n",
    "\n",
    "y_pred_test_norm = modelo_autoformer.predict(test_p[0])\n",
    "\n",
    "y_pred_test = scaler_pasajeros.inverse_transform(y_pred_test_norm.reshape(-1, 1)).flatten()\n",
    "y_test_real = scaler_pasajeros.inverse_transform(test_p[1].reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test_real, y_pred_test))\n",
    "mae = mean_absolute_error(y_test_real, y_pred_test)\n",
    "epsilon = 1e-10\n",
    "mape = np.mean(np.abs((y_test_real - y_pred_test) / (y_test_real + epsilon))) * 100\n",
    "ss_total = np.sum((y_test_real - np.mean(y_test_real)) ** 2)\n",
    "ss_residual = np.sum((y_test_real - y_pred_test) ** 2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(\"\\n===== Metricas de rendimiento Modelo Autoformer =====\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\n=== Visualizando resultados ===\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Perdida durante entrenamiento (Loss)')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train')\n",
    "plt.plot(history.history['val_mae'], label='Validation')\n",
    "plt.title('MAE durante entrenamiento')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(test_dates, y_test_real, 'b-', label='Real', linewidth=2)\n",
    "plt.plot(test_dates, y_pred_test, 'r--', label='Predicho (Autoformer)', linewidth=2)\n",
    "plt.title('Prediccion vs Real - Conjunto de Test')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Total Pasajeros')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "metrics_text = f\"RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nMAPE: {mape:.2f}%\\nR²: {r2:.2f}\"\n",
    "plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "daily_error = y_test_real - y_pred_test\n",
    "daily_pct_error = (daily_error / (y_test_real + 1e-10)) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(test_dates, daily_error, 'g-', linewidth=1.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Error de Prediccin a lo largo del Tiempo')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Error (Real - Predicho)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(daily_error, bins=30, color='teal', alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Distribucion del Error')\n",
    "plt.xlabel('Error de Prediccion')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'fecha': test_dates,\n",
    "    'real': y_test_real,\n",
    "    'predicho': y_pred_test,\n",
    "    'error': daily_error,\n",
    "    'error_porcentual': daily_pct_error\n",
    "})\n",
    "\n",
    "print(\"\\n=== Resumen de los primeros registros de resultados ===\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"\\nProceso completado, Modelo Autoformer sido entrenado y evaluado.\")\n",
    "\n",
    "try:\n",
    "    lstm_results = pd.read_csv('/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Resultados/lstm_model_pasajeros_results.csv')\n",
    "    lstm_results['date'] = pd.to_datetime(lstm_results['date'])\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'fecha': test_dates,\n",
    "        'real': y_test_real,\n",
    "        'lstm_pred': lstm_results['predicted'].values,\n",
    "        'autoformer_pred': y_pred_test\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['real'], 'b-', label='Real', linewidth=2)\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['lstm_pred'], 'r--', label='LSTM', linewidth=1.5)\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['autoformer_pred'], 'g--', label='Autoformer', linewidth=1.5)\n",
    "    plt.title('Comparacion: LSTM vs Autoformer')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Total Pasajeros')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    lstm_rmse = sqrt(mean_squared_error(comparison_df['real'], comparison_df['lstm_pred']))\n",
    "    autoformer_rmse = sqrt(mean_squared_error(comparison_df['real'], comparison_df['autoformer_pred']))\n",
    "\n",
    "    print(\"\\n=== Comparacion de Modelos ===\")\n",
    "    print(f\"RMSE LSTM: {lstm_rmse:.4f}\")\n",
    "    print(f\"RMSE Autoformer: {autoformer_rmse:.4f}\")\n",
    "    print(f\"Mejora: {((lstm_rmse - autoformer_rmse) / lstm_rmse) * 100:.2f}%\")\n",
    "except:\n",
    "    print(\"\\nNo se encontraron resultados previos del LSTM para comparar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNvthZLOIlCp"
   },
   "source": [
    "# AUTOFORMER VERSION MEJORADA (Pasajeros y Vehiculos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 304078,
     "status": "ok",
     "timestamp": 1745474968119,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "WC_V4u0Ccq-S",
    "outputId": "1d1cdc47-ff57-4490-d9e0-7642be069cff"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Add\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"AutoformerTS\")\n",
    "\n",
    "def cargar_datos(ruta_csv):\n",
    "    logger.info(f\"Cargando datos desde: {ruta_csv}\")\n",
    "    df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "    df[\"con_fecha\"] = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "    df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "    df['fecha_embarque'] = pd.to_datetime(df['fecha_embarque'], errors='coerce')\n",
    "\n",
    "    columnas_a_mantener = [\n",
    "        'fecha_embarque','tipo_agrupacion',\n",
    "        'dia_del_anio_sin', 'dia_del_anio_cos',\n",
    "        'dia_embarque_sin', 'dia_embarque_cos',\n",
    "        'hora_embarque_sin', 'hora_embarque_cos',\n",
    "        'is_weekend',\n",
    "        'mes_embarque_sin', 'mes_embarque_cos',\n",
    "        'week_of_year_sin', 'week_of_year_cos',\n",
    "        'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday',\n",
    "        'weekday_sin', 'weekday_cos'\n",
    "    ]\n",
    "\n",
    "    df = df[columnas_a_mantener]\n",
    "    df_pasajeros = df[df[\"tipo_agrupacion\"] == 1].copy()\n",
    "    df_vehiculos = df[df[\"tipo_agrupacion\"] == 0].copy()\n",
    "\n",
    "    logger.info(f\"Datos cargados: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "    return df, df_pasajeros, df_vehiculos\n",
    "\n",
    "def procesar_dataset_completo(df_original, df_filtrado, nombre_target):\n",
    "\n",
    "    logger.info(f\"Procesando dataset para {nombre_target}\")\n",
    "\n",
    "    rango_fechas = pd.date_range(\n",
    "        start=df_original['fecha_embarque'].min(),\n",
    "        end=df_original['fecha_embarque'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    df_daily = (\n",
    "        df_filtrado.groupby('fecha_embarque')\n",
    "        .size()\n",
    "        .reindex(rango_fechas, fill_value=0)\n",
    "        .reset_index(name=nombre_target)\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    df_temp = (\n",
    "        df_original.drop(columns=['tipo_agrupacion'])\n",
    "        .drop_duplicates('fecha_embarque')\n",
    "        .set_index('fecha_embarque')\n",
    "        .reindex(rango_fechas)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    cols_categoricas = [\n",
    "        'is_weekend', 'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi', 'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday'\n",
    "    ]\n",
    "\n",
    "    for col in cols_categoricas:\n",
    "        df_temp[col] = df_temp[col].ffill().fillna(0)\n",
    "\n",
    "    df_temp['dia_del_anio'] = df_temp['fecha_embarque'].dt.dayofyear\n",
    "    df_temp['dia_del_anio_sin'] = np.sin(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "    df_temp['dia_del_anio_cos'] = np.cos(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "\n",
    "    df_temp['dia_embarque'] = df_temp['fecha_embarque'].dt.day\n",
    "    df_temp['dia_embarque_sin'] = np.sin(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "    df_temp['dia_embarque_cos'] = np.cos(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "\n",
    "    df_temp['mes_embarque'] = df_temp['fecha_embarque'].dt.month\n",
    "    df_temp['mes_embarque_sin'] = np.sin(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "    df_temp['mes_embarque_cos'] = np.cos(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "\n",
    "    df_temp['week_of_year'] = df_temp['fecha_embarque'].dt.isocalendar().week\n",
    "    df_temp['week_of_year_sin'] = np.sin(2 * np.pi * df_temp['week_of_year']/52)\n",
    "    df_temp['week_of_year_cos'] = np.cos(2 * np.pi * df_temp['week_of_year']/52)\n",
    "\n",
    "    if df_temp['weekday_sin'].isna().any() or df_temp['weekday_cos'].isna().any():\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp = df_temp.drop(columns=['weekday'])\n",
    "\n",
    "    df_temp = df_temp.drop(columns=['dia_del_anio', 'dia_embarque', 'mes_embarque', 'week_of_year'])\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_temp,\n",
    "        df_daily,\n",
    "        on='fecha_embarque',\n",
    "        how='left'\n",
    "    ).fillna({nombre_target: 0})\n",
    "\n",
    "    df_final = df_final.sort_values('fecha_embarque')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f'{nombre_target}_norm'] = scaler.fit_transform(df_final[[nombre_target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    return df_final, scaler\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7):\n",
    "    features = df.columns.difference(['fecha_embarque', target_col, f'{target_col}_norm'])\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(df)):\n",
    "        X_seq = df.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X.append(X_seq)\n",
    "        y.append(df.iloc[i][f'{target_col}_norm'])\n",
    "\n",
    "    logger.info(f\"Secuencias creadas: {len(X)} secuencias con lookback={lookback}\")\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    assert sum(ratios) == 1.0, \"Las proporciones deben sumar 1\"\n",
    "\n",
    "    train_end = int(len(X) * ratios[0])\n",
    "    val_end = train_end + int(len(X) * ratios[1])\n",
    "\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "    logger.info(f\"Division temporal: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "class SeriesDecomposition(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.norm = None\n",
    "        self.avg_pool = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.avg_pool = tf.keras.layers.AveragePooling1D(\n",
    "            pool_size=self.kernel_size,\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_norm = self.norm(x)\n",
    "        trend = self.avg_pool(x_norm)\n",
    "        seasonal = x_norm - trend\n",
    "        return trend, seasonal\n",
    "\n",
    "class AutoCorrelationMechanism(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = None\n",
    "        self.norm = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.d_model//self.num_heads\n",
    "        )\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_norm = self.norm(x)\n",
    "        attn_output = self.mha(x_norm, x_norm, x_norm)\n",
    "        return attn_output\n",
    "\n",
    "class AutoformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff=128, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.autocorr = None\n",
    "        self.decomp1 = None\n",
    "        self.decomp2 = None\n",
    "        self.ff = None\n",
    "        self.dropout = None\n",
    "        self.norm = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.autocorr = AutoCorrelationMechanism(self.d_model, self.num_heads)\n",
    "        self.decomp1 = SeriesDecomposition(kernel_size=7)\n",
    "        self.decomp2 = SeriesDecomposition(kernel_size=7)\n",
    "\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            Dense(self.d_ff, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(self.d_model, kernel_regularizer=l2(1e-5))\n",
    "        ])\n",
    "\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        attn_out = self.autocorr(x)\n",
    "        attn_out = self.dropout(attn_out, training=training)\n",
    "        x1 = x + attn_out\n",
    "\n",
    "        trend1, seasonal1 = self.decomp1(x1)\n",
    "\n",
    "        ff_out = self.ff(seasonal1, training=training)\n",
    "        ff_out = self.dropout(ff_out, training=training)\n",
    "        x2 = seasonal1 + ff_out\n",
    "\n",
    "        trend2, seasonal2 = self.decomp2(x2)\n",
    "\n",
    "        return trend1 + trend2, seasonal2\n",
    "\n",
    "\n",
    "def crear_autoformer_optimizado(seq_len, n_features, d_model=64, num_heads=4, num_blocks=2, dropout_rate=0.1):\n",
    "\n",
    "    inputs = Input(shape=(seq_len, n_features), name=\"input_sequences\")\n",
    "\n",
    "    x = Conv1D(d_model, kernel_size=1, padding='same', name=\"projection\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    decomp = SeriesDecomposition(kernel_size=7)\n",
    "    trend_init, seasonal_init = decomp(x)\n",
    "\n",
    "    trend, seasonal = trend_init, seasonal_init\n",
    "    trend_outputs = [trend]\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        t, s = AutoformerBlock(\n",
    "            d_model, num_heads, d_ff=d_model*2, dropout_rate=dropout_rate\n",
    "        )(seasonal)\n",
    "        trend = trend + t\n",
    "        seasonal = s\n",
    "        trend_outputs.append(trend)\n",
    "\n",
    "    trend_concat = Concatenate(axis=-1)(trend_outputs)\n",
    "\n",
    "    combined = Conv1D(d_model, kernel_size=1, padding='same')(trend_concat)\n",
    "    combined = BatchNormalization()(combined)\n",
    "\n",
    "    final_rep = Add()([combined, seasonal])\n",
    "\n",
    "    pooled = GlobalAveragePooling1D()(final_rep)\n",
    "\n",
    "    x = Dense(d_model, activation='relu', kernel_regularizer=l2(1e-4))(pooled)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name=\"prediction\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"Autoformer\")\n",
    "\n",
    "    optimizer = Adam(\n",
    "        learning_rate=5e-4,\n",
    "        clipnorm=0.5,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def entrenar_modelo(model, X_train, y_train, X_val, y_val,\n",
    "                   batch_size=32, epochs=100, patience=15,\n",
    "                   model_path=None, verbose=1):\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=patience//2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if model_path:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        # Asegurar que termina en .keras\n",
    "        if not model_path.endswith('.keras'):\n",
    "            model_path = f\"{model_path}.keras\"\n",
    "\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=model_path,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    return history, model\n",
    "\n",
    "def evaluar_modelo(model, X_test, y_test, scaler, col_target,\n",
    "                  fechas_test=None, titulo=\"Autoformer\"):\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "    rmse_norm = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae_norm = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    eps = 1e-8\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), eps))) * 100\n",
    "\n",
    "    if hasattr(scaler, 'data_min_'): \n",
    "        y_test_reshaped = y_test.reshape(-1, 1)\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "        y_test_orig = scaler.inverse_transform(y_test_reshaped).flatten()\n",
    "        y_pred_orig = scaler.inverse_transform(y_pred_reshaped).flatten()\n",
    "\n",
    "        rmse_orig = sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
    "        mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "\n",
    "        mape_orig = np.mean(np.abs((y_test_orig - y_pred_orig) / np.maximum(np.abs(y_test_orig), eps))) * 100\n",
    "    else:\n",
    "        y_test_orig = y_test\n",
    "        y_pred_orig = y_pred\n",
    "        rmse_orig = rmse_norm\n",
    "        mae_orig = mae_norm\n",
    "        mape_orig = mape\n",
    "\n",
    "    print(f\"\\n===== {titulo} Performance =====\")\n",
    "    print(f\"Metricas normalizadas:\")\n",
    "    print(f\"RMSE: {rmse_norm:.4f}\")\n",
    "    print(f\"MAE:  {mae_norm:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "    print(f\"\\nMetricas originales para {col_target}:\")\n",
    "    print(f\"RMSE: {rmse_orig:.2f}\")\n",
    "    print(f\"MAE:  {mae_orig:.2f}\")\n",
    "    print(f\"MAPE: {mape_orig:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    if fechas_test is not None and len(fechas_test) == len(y_test_orig):\n",
    "        plt.plot(fechas_test, y_test_orig, label='Real', color='blue')\n",
    "        plt.plot(fechas_test, y_pred_orig, label='Prediccion', linestyle='--', color='red')\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    else:\n",
    "        plt.plot(y_test_orig, label='Real', color='blue')\n",
    "        plt.plot(y_pred_orig, label='Predicción', linestyle='--', color='red')\n",
    "\n",
    "    plt.title(f\"Prediccion de {col_target} con {titulo}\")\n",
    "    plt.xlabel(\"Fecha\")\n",
    "    plt.ylabel(col_target)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    metrics = {\n",
    "        'rmse_norm': rmse_norm,\n",
    "        'mae_norm': mae_norm,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'rmse_orig': rmse_orig,\n",
    "        'mae_orig': mae_orig,\n",
    "        'mape_orig': mape_orig\n",
    "    }\n",
    "\n",
    "    return metrics, y_pred_orig\n",
    "\n",
    "def guardar_modelo(model, scaler, ruta_base):\n",
    "    os.makedirs(os.path.dirname(ruta_base), exist_ok=True)\n",
    "\n",
    "    if not ruta_base.endswith('.keras'):\n",
    "        ruta_modelo = f\"{ruta_base}.keras\"\n",
    "    else:\n",
    "        ruta_modelo = ruta_base\n",
    "    model.save(ruta_modelo)\n",
    "\n",
    "    ruta_scaler = f\"{os.path.splitext(ruta_base)[0]}_scaler.pkl\"\n",
    "\n",
    "    with open(ruta_scaler, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    logger.info(f\"Modelo guardado en {ruta_modelo}\")\n",
    "    logger.info(f\"Scaler guardado en {ruta_scaler}\")\n",
    "\n",
    "def cargar_modelo(ruta_base):\n",
    "    if not ruta_base.endswith('.keras'):\n",
    "        ruta_modelo = f\"{ruta_base}.keras\"\n",
    "    else:\n",
    "        ruta_modelo = ruta_base\n",
    "        ruta_base = os.path.splitext(ruta_base)[0]\n",
    "\n",
    "    model = tf.keras.models.load_model(ruta_modelo)\n",
    "\n",
    "    ruta_scaler = f\"{ruta_base}_scaler.pkl\"\n",
    "\n",
    "    with open(ruta_scaler, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    logger.info(f\"Modelo cargado desde {ruta_modelo}\")\n",
    "    return model, scaler\n",
    "\n",
    "\n",
    "def procesar_y_entrenar(ruta_csv, lookback=7, batch_size=32, epochs=100,\n",
    "                       target='pasajeros', guardar=True, ruta_guardado=None):\n",
    "    logger.info(f\"Iniciando procesamiento para {target} con lookback={lookback}\")\n",
    "\n",
    "    df, df_pasajeros, df_vehiculos = cargar_datos(ruta_csv)\n",
    "\n",
    "    if target == 'pasajeros':\n",
    "        df_procesado, scaler = procesar_dataset_completo(df, df_pasajeros, 'total_pasajeros')\n",
    "        col_target = 'total_pasajeros'\n",
    "    else:\n",
    "        df_procesado, scaler = procesar_dataset_completo(df, df_vehiculos, 'total_vehiculos')\n",
    "        col_target = 'total_vehiculos'\n",
    "\n",
    "    X, y = crear_secuencias(df_procesado, col_target, lookback)\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = split_temporal(X, y)\n",
    "\n",
    "    fechas_test = df_procesado['fecha_embarque'].iloc[-len(X_test):].values\n",
    "\n",
    "    n_features = X_train.shape[2]\n",
    "    model = crear_autoformer_optimizado(\n",
    "        seq_len=lookback,\n",
    "        n_features=n_features,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        num_blocks=2,\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model_path = None\n",
    "    if guardar and ruta_guardado:\n",
    "        os.makedirs(os.path.dirname(ruta_guardado), exist_ok=True)\n",
    "        if not ruta_guardado.endswith('.keras'):\n",
    "            model_path = f\"{ruta_guardado}.keras\"\n",
    "        else:\n",
    "            model_path = ruta_guardado\n",
    "\n",
    "    history, model = entrenar_modelo(\n",
    "        model, X_train, y_train, X_val, y_val,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        patience=15,\n",
    "        model_path=model_path\n",
    "    )\n",
    "\n",
    "    metrics, y_pred = evaluar_modelo(\n",
    "        model, X_test, y_test, scaler, col_target,\n",
    "        fechas_test=fechas_test,\n",
    "        titulo=f\"Autoformer para {target}\"\n",
    "    )\n",
    "\n",
    "    if guardar and ruta_guardado and model_path is not None:\n",
    "        guardar_modelo(model, scaler, model_path)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title('Loss durante entrenamiento')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='train')\n",
    "    plt.plot(history.history['val_mae'], label='validation')\n",
    "    plt.title('MAE durante entrenamiento')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, metrics, y_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\"\n",
    "\n",
    "    os.makedirs(\"modelos\", exist_ok=True)\n",
    "\n",
    "    model_pasajeros, metrics_pasajeros, pred_pasajeros = procesar_y_entrenar(\n",
    "        ruta_csv,\n",
    "        lookback=7,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        target='pasajeros',\n",
    "        guardar=True,\n",
    "        ruta_guardado='modelos/autoformer_pasajeros.keras'\n",
    "    )\n",
    "\n",
    "    model_vehiculos, metrics_vehiculos, pred_vehiculos = procesar_y_entrenar(\n",
    "        ruta_csv,\n",
    "        lookback=7,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        target='vehiculos',\n",
    "        guardar=True,\n",
    "        ruta_guardado='modelos/autoformer_vehiculos.keras'\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== COMPARACION RESULTADOS =====\")\n",
    "    print(\"Metrica    | Pasajeros | Vehículos\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"RMSE      | {metrics_pasajeros['rmse_orig']:.2f} | {metrics_vehiculos['rmse_orig']:.2f}\")\n",
    "    print(f\"MAE       | {metrics_pasajeros['mae_orig']:.2f} | {metrics_vehiculos['mae_orig']:.2f}\")\n",
    "    print(f\"MAPE (%)  | {metrics_pasajeros['mape_orig']:.2f} | {metrics_vehiculos['mape_orig']:.2f}\")\n",
    "    print(f\"R²        | {metrics_pasajeros['r2']:.4f} | {metrics_vehiculos['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 107497,
     "status": "ok",
     "timestamp": 1744712816780,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "y2Xsv1q6-ya8",
    "outputId": "641556d7-3bdb-4805-c53f-9a1887e7dd8e"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, TimeDistributed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "\n",
    "ruta_csv = \"/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\"\n",
    "df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "df[\"con_fecha\"] = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "df['fecha_embarque'] = pd.to_datetime(df['fecha_embarque'], errors='coerce')\n",
    "\n",
    "columnas_a_mantener = [\n",
    "    'fecha_embarque','tipo_agrupacion',\n",
    "    'dia_del_anio_sin', 'dia_del_anio_cos',\n",
    "    'dia_embarque_sin', 'dia_embarque_cos',\n",
    "    'hora_embarque_sin', 'hora_embarque_cos',\n",
    "    'is_weekend',\n",
    "    'mes_embarque_sin', 'mes_embarque_cos',\n",
    "    'week_of_year_sin', 'week_of_year_cos',\n",
    "    'season_1', 'season_2', 'season_3', 'season_4',\n",
    "    'is_festivo_nacional', 'is_festivo_local',\n",
    "    'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "    'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "    'is_mawlid_nabi',\n",
    "    'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "    'is_friday', 'is_saturday', 'is_sunday',\n",
    "    'weekday_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "df = df[columnas_a_mantener]\n",
    "df_pasajeros = df[df[\"tipo_agrupacion\"] == 1].copy()\n",
    "\n",
    "def procesar_dataset_completo(df_original, df_filtrado, nombre_target):\n",
    "\n",
    "    rango_fechas = pd.date_range(\n",
    "        start=df_original['fecha_embarque'].min(),\n",
    "        end=df_original['fecha_embarque'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    df_daily = (\n",
    "        df_filtrado.groupby('fecha_embarque')\n",
    "        .size()\n",
    "        .reindex(rango_fechas, fill_value=0)\n",
    "        .reset_index(name=nombre_target)\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    df_temp = (\n",
    "        df_original.drop(columns=['tipo_agrupacion'])\n",
    "        .drop_duplicates('fecha_embarque')\n",
    "        .set_index('fecha_embarque')\n",
    "        .reindex(rango_fechas)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    cols_categoricas = [\n",
    "        'is_weekend', 'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi', 'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday'\n",
    "    ]\n",
    "\n",
    "    for col in cols_categoricas:\n",
    "        df_temp[col] = df_temp[col].fillna(method='ffill').fillna(0)\n",
    "\n",
    "    df_temp['dia_del_anio'] = df_temp['fecha_embarque'].dt.dayofyear\n",
    "    df_temp['dia_del_anio_sin'] = np.sin(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "    df_temp['dia_del_anio_cos'] = np.cos(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "\n",
    "    df_temp['dia_embarque'] = df_temp['fecha_embarque'].dt.day\n",
    "    df_temp['dia_embarque_sin'] = np.sin(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "    df_temp['dia_embarque_cos'] = np.cos(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "\n",
    "    df_temp['mes_embarque'] = df_temp['fecha_embarque'].dt.month\n",
    "    df_temp['mes_embarque_sin'] = np.sin(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "    df_temp['mes_embarque_cos'] = np.cos(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "\n",
    "    df_temp['week_of_year'] = df_temp['fecha_embarque'].dt.isocalendar().week\n",
    "    df_temp['week_of_year_sin'] = np.sin(2 * np.pi * df_temp['week_of_year']/52)\n",
    "    df_temp['week_of_year_cos'] = np.cos(2 * np.pi * df_temp['week_of_year']/52)\n",
    "\n",
    "    if df_temp['weekday_sin'].isna().any() or df_temp['weekday_cos'].isna().any():\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp = df_temp.drop(columns=['weekday'])\n",
    "\n",
    "    df_temp = df_temp.drop(columns=['dia_del_anio', 'dia_embarque', 'mes_embarque', 'week_of_year'])\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_temp,\n",
    "        df_daily,\n",
    "        on='fecha_embarque',\n",
    "        how='left'\n",
    "    ).fillna({nombre_target: 0})\n",
    "\n",
    "    df_final = df_final.sort_values('fecha_embarque')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f'{nombre_target}_norm'] = scaler.fit_transform(df_final[[nombre_target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    return df_final, scaler\n",
    "\n",
    "df_pasajeros_daily, scaler_pasajeros = procesar_dataset_completo(df, df_pasajeros, 'total_pasajeros')\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7):\n",
    "    features = df.columns.difference(['fecha_embarque', target_col, f'{target_col}_norm'])\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(df)):\n",
    "        X_seq = df.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X.append(X_seq)\n",
    "        y.append(df.iloc[i][f'{target_col}_norm'])\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "LOOKBACK = 7\n",
    "X_pasajeros, y_pasajeros = crear_secuencias(df_pasajeros_daily, 'total_pasajeros', LOOKBACK)\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    train_end = int(len(X) * ratios[0])\n",
    "    val_end = train_end + int(len(X) * ratios[1])\n",
    "    return (X[:train_end], y[:train_end]), (X[train_end:val_end], y[train_end:val_end]), (X[val_end:], y[val_end:])\n",
    "\n",
    "(train_p, val_p, test_p) = split_temporal(X_pasajeros, y_pasajeros)\n",
    "\n",
    "train_end_p = int(len(X_pasajeros) * 0.7)\n",
    "val_end_p = train_end_p + int(len(X_pasajeros) * 0.15)\n",
    "test_dates = df_pasajeros_daily['fecha_embarque'].iloc[-len(test_p[0]):].values\n",
    "\n",
    "print(\"\\n=== Preparacion de datos completada ===\")\n",
    "print(f\"Train set: {train_p[0].shape}\")\n",
    "print(f\"Validation set: {val_p[0].shape}\")\n",
    "print(f\"Test set: {test_p[0].shape}\")\n",
    "\n",
    "class SeriesDecomposition(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, kernel_size=25):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_trend = Conv1D(filters=1, kernel_size=self.kernel_size,\n",
    "                                 padding='same', strides=1, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        trend = self.conv_trend(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class AutoCorrelationBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        attn_output = self.mha(x, x, x)\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        return self.ln(x + attn_output)\n",
    "\n",
    "class AutoformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, kernel_size=25, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.decomp = SeriesDecomposition(kernel_size=kernel_size)\n",
    "        self.auto_corr = AutoCorrelationBlock(d_model, num_heads, dropout=dropout)\n",
    "\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        seasonal_part, trend_part = self.decomp(x)\n",
    "\n",
    "        seasonal_part = self.auto_corr(seasonal_part, training=training)\n",
    "\n",
    "        x_out = seasonal_part + trend_part\n",
    "        x_out2 = self.ff(x_out)\n",
    "        x_out2 = self.dropout(x_out2, training=training)\n",
    "        x_out = self.ln(x_out + x_out2)\n",
    "        return x_out\n",
    "\n",
    "class AutoformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, kernel_size=25, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            AutoformerEncoderLayer(d_model, num_heads, d_ff, kernel_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        position = np.arange(0, sequence_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pos_encoding = np.zeros((sequence_length, d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding\n",
    "\n",
    "def crear_modelo_autoformer(\n",
    "    input_shape,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=128,\n",
    "    num_encoder_layers=2,\n",
    "    kernel_size=25,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = TimeDistributed(Dense(d_model))(inputs)\n",
    "\n",
    "    x = PositionalEncoding(input_shape[0], d_model)(x)\n",
    "\n",
    "    x = AutoformerEncoder(\n",
    "        num_layers=num_encoder_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout_rate\n",
    "    )(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def crear_callbacks(model_name):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{model_name}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    return [early_stopping, checkpoint, reduce_lr]\n",
    "\n",
    "print(\"\\n=== Creando y entrenando el modelo Autoformer pasajeros ===\")\n",
    "\n",
    "input_shape = (train_p[0].shape[1], train_p[0].shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "modelo_autoformer = crear_modelo_autoformer(\n",
    "    input_shape=input_shape,\n",
    "    d_model=64,         \n",
    "    num_heads=4,        \n",
    "    d_ff=128,           \n",
    "    num_encoder_layers=2,\n",
    "    kernel_size=25,     \n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "modelo_autoformer.summary()\n",
    "\n",
    "callbacks = crear_callbacks(\"autoformer_model_pasajeros\")\n",
    "\n",
    "history = modelo_autoformer.fit(\n",
    "    train_p[0], train_p[1],\n",
    "    validation_data=(val_p[0], val_p[1]),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluando modelo Autoformer pasajeros ===\")\n",
    "\n",
    "y_pred_test_norm = modelo_autoformer.predict(test_p[0])\n",
    "\n",
    "y_pred_test = scaler_pasajeros.inverse_transform(y_pred_test_norm.reshape(-1, 1)).flatten()\n",
    "y_test_real = scaler_pasajeros.inverse_transform(test_p[1].reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test_real, y_pred_test))\n",
    "mae = mean_absolute_error(y_test_real, y_pred_test)\n",
    "epsilon = 1e-10\n",
    "mape = np.mean(np.abs((y_test_real - y_pred_test) / (y_test_real + epsilon))) * 100\n",
    "ss_total = np.sum((y_test_real - np.mean(y_test_real)) ** 2)\n",
    "ss_residual = np.sum((y_test_real - y_pred_test) ** 2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(\"\\n===== Metricas de Rendimiento Modelo Autoformer =====\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\n=== Visualizando resultados ===\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Perdida durante entrenamiento (Loss)')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train')\n",
    "plt.plot(history.history['val_mae'], label='Validation')\n",
    "plt.title('MAE durante entrenamiento')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(test_dates, y_test_real, 'b-', label='Real', linewidth=2)\n",
    "plt.plot(test_dates, y_pred_test, 'r--', label='Predicho (Autoformer)', linewidth=2)\n",
    "plt.title('Prediccion vs Real - Conjunto de Test')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Total Pasajeros')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "metrics_text = f\"RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nMAPE: {mape:.2f}%\\nR²: {r2:.2f}\"\n",
    "plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "daily_error = y_test_real - y_pred_test\n",
    "daily_pct_error = (daily_error / (y_test_real + 1e-10)) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(test_dates, daily_error, 'g-', linewidth=1.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Error de Prediccion a lo largo del Tiempo')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Error (Real - Predicho)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(daily_error, bins=30, color='teal', alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Distribucion del Error')\n",
    "plt.xlabel('Error de Prediccion')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'fecha': test_dates,\n",
    "    'real': y_test_real,\n",
    "    'predicho': y_pred_test,\n",
    "    'error': daily_error,\n",
    "    'error_porcentual': daily_pct_error\n",
    "})\n",
    "\n",
    "print(\"\\n=== Resumen de los primeros registros de resultados ===\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"\\nProceso completado El modelo Autoformer ha sido entrenado y evaluado con éxito.\")\n",
    "\n",
    "try:\n",
    "    lstm_results = pd.read_csv('/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Resultados/lstm_model_pasajeros_results.csv')\n",
    "    lstm_results['date'] = pd.to_datetime(lstm_results['date'])\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'fecha': test_dates,\n",
    "        'real': y_test_real,\n",
    "        'lstm_pred': lstm_results['predicted'].values,\n",
    "        'autoformer_pred': y_pred_test\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['real'], 'b-', label='Real', linewidth=2)\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['lstm_pred'], 'r--', label='LSTM', linewidth=1.5)\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['autoformer_pred'], 'g--', label='Autoformer', linewidth=1.5)\n",
    "    plt.title('Comparacion: LSTM vs Autoformer')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Total Pasajeros')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    lstm_rmse = sqrt(mean_squared_error(comparison_df['real'], comparison_df['lstm_pred']))\n",
    "    autoformer_rmse = sqrt(mean_squared_error(comparison_df['real'], comparison_df['autoformer_pred']))\n",
    "\n",
    "    print(\"\\n=== Comparacion Modelos ===\")\n",
    "    print(f\"RMSE LSTM: {lstm_rmse:.4f}\")\n",
    "    print(f\"RMSE Autoformer: {autoformer_rmse:.4f}\")\n",
    "    print(f\"Mejora: {((lstm_rmse - autoformer_rmse) / lstm_rmse) * 100:.2f}%\")\n",
    "except:\n",
    "    print(\"\\nNo se encontraron resultados previos del LSTM para comparar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flSLnrfUkOcJ"
   },
   "source": [
    "# SOLO PASAJEROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 120626,
     "status": "ok",
     "timestamp": 1744722773180,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "S7uWIl1ueHKH",
    "outputId": "d1b70c08-2ee7-4885-c1c8-ef4018256d2e"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, TimeDistributed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "\n",
    "ruta_csv = \"/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\"\n",
    "df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "df[\"con_fecha\"] = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "df['fecha_embarque'] = pd.to_datetime(df['fecha_embarque'], errors='coerce')\n",
    "\n",
    "columnas_a_mantener = [\n",
    "    'fecha_embarque','tipo_agrupacion',\n",
    "    'dia_del_anio_sin', 'dia_del_anio_cos',\n",
    "    'dia_embarque_sin', 'dia_embarque_cos',\n",
    "    'hora_embarque_sin', 'hora_embarque_cos',\n",
    "    'is_weekend',\n",
    "    'mes_embarque_sin', 'mes_embarque_cos',\n",
    "    'week_of_year_sin', 'week_of_year_cos',\n",
    "    'season_1', 'season_2', 'season_3', 'season_4',\n",
    "    'is_festivo_nacional', 'is_festivo_local',\n",
    "    'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "    'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "    'is_mawlid_nabi',\n",
    "    'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "    'is_friday', 'is_saturday', 'is_sunday',\n",
    "    'weekday_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "df = df[columnas_a_mantener]\n",
    "df_pasajeros = df[df[\"tipo_agrupacion\"] == 1].copy()\n",
    "\n",
    "def procesar_dataset_completo(df_original, df_filtrado, nombre_target):\n",
    "    rango_fechas = pd.date_range(\n",
    "        start=df_original['fecha_embarque'].min(),\n",
    "        end=df_original['fecha_embarque'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    df_daily = (\n",
    "        df_filtrado.groupby('fecha_embarque')\n",
    "        .size()\n",
    "        .reindex(rango_fechas, fill_value=0)\n",
    "        .reset_index(name=nombre_target)\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    df_temp = (\n",
    "        df_original.drop(columns=['tipo_agrupacion'])\n",
    "        .drop_duplicates('fecha_embarque')\n",
    "        .set_index('fecha_embarque')\n",
    "        .reindex(rango_fechas)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    cols_categoricas = [\n",
    "        'is_weekend', 'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi', 'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday'\n",
    "    ]\n",
    "\n",
    "    for col in cols_categoricas:\n",
    "        df_temp[col] = df_temp[col].fillna(method='ffill').fillna(0)\n",
    "\n",
    "    df_temp['dia_del_anio'] = df_temp['fecha_embarque'].dt.dayofyear\n",
    "    df_temp['dia_del_anio_sin'] = np.sin(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "    df_temp['dia_del_anio_cos'] = np.cos(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "\n",
    "    df_temp['dia_embarque'] = df_temp['fecha_embarque'].dt.day\n",
    "    df_temp['dia_embarque_sin'] = np.sin(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "    df_temp['dia_embarque_cos'] = np.cos(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "\n",
    "    df_temp['mes_embarque'] = df_temp['fecha_embarque'].dt.month\n",
    "    df_temp['mes_embarque_sin'] = np.sin(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "    df_temp['mes_embarque_cos'] = np.cos(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "\n",
    "    df_temp['week_of_year'] = df_temp['fecha_embarque'].dt.isocalendar().week\n",
    "    df_temp['week_of_year_sin'] = np.sin(2 * np.pi * df_temp['week_of_year']/52)\n",
    "    df_temp['week_of_year_cos'] = np.cos(2 * np.pi * df_temp['week_of_year']/52)\n",
    "\n",
    "    if df_temp['weekday_sin'].isna().any() or df_temp['weekday_cos'].isna().any():\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday  \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp = df_temp.drop(columns=['weekday'])\n",
    "\n",
    "    df_temp = df_temp.drop(columns=['dia_del_anio', 'dia_embarque', 'mes_embarque', 'week_of_year'])\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_temp,\n",
    "        df_daily,\n",
    "        on='fecha_embarque',\n",
    "        how='left'\n",
    "    ).fillna({nombre_target: 0})\n",
    "\n",
    "    df_final = df_final.sort_values('fecha_embarque')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f'{nombre_target}_norm'] = scaler.fit_transform(df_final[[nombre_target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    return df_final, scaler\n",
    "\n",
    "df_pasajeros_daily, scaler_pasajeros = procesar_dataset_completo(df, df_pasajeros, 'total_pasajeros')\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7):\n",
    "    features = df.columns.difference(['fecha_embarque', target_col, f'{target_col}_norm'])\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(df)):\n",
    "        X_seq = df.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X.append(X_seq)\n",
    "        y.append(df.iloc[i][f'{target_col}_norm'])\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "LOOKBACK = 7\n",
    "X_pasajeros, y_pasajeros = crear_secuencias(df_pasajeros_daily, 'total_pasajeros', LOOKBACK)\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    train_end = int(len(X) * ratios[0])\n",
    "    val_end = train_end + int(len(X) * ratios[1])\n",
    "    return (X[:train_end], y[:train_end]), (X[train_end:val_end], y[train_end:val_end]), (X[val_end:], y[val_end:])\n",
    "\n",
    "(train_p, val_p, test_p) = split_temporal(X_pasajeros, y_pasajeros)\n",
    "\n",
    "train_end_p = int(len(X_pasajeros) * 0.7)\n",
    "val_end_p = train_end_p + int(len(X_pasajeros) * 0.15)\n",
    "test_dates = df_pasajeros_daily['fecha_embarque'].iloc[-len(test_p[0]):].values\n",
    "\n",
    "print(\"\\n=== Preparacion de datos completada ===\")\n",
    "print(f\"Train set: {train_p[0].shape}\")\n",
    "print(f\"Validation set: {val_p[0].shape}\")\n",
    "print(f\"Test set: {test_p[0].shape}\")\n",
    "\n",
    "class SeriesDecomposition(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, kernel_size=25, dilation_rate=2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.conv_trend = Conv1D(filters=1, kernel_size=self.kernel_size,\n",
    "                                 dilation_rate=self.dilation_rate,\n",
    "                                 padding='same', strides=1, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        trend = self.conv_trend(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class AutoCorrelationBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        attn_output = self.mha(x, x, x)\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        return self.ln(x + attn_output)\n",
    "\n",
    "class AutoformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, kernel_size=25, dropout=0.1, dilation_rate=2):\n",
    "        super().__init__()\n",
    "        self.decomp = SeriesDecomposition(kernel_size=kernel_size, dilation_rate=dilation_rate)\n",
    "        self.auto_corr = AutoCorrelationBlock(d_model, num_heads, dropout=dropout)\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        seasonal_part, trend_part = self.decomp(x)\n",
    "        seasonal_part = self.auto_corr(seasonal_part, training=training)\n",
    "        x_out = seasonal_part + trend_part\n",
    "        x_out2 = self.ff(x_out)\n",
    "        x_out2 = self.dropout(x_out2, training=training)\n",
    "        x_out = self.ln(x_out + x_out2)\n",
    "        return x_out\n",
    "\n",
    "class AutoformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, kernel_size=25, dropout=0.1, dilation_rate=2):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            AutoformerEncoderLayer(d_model, num_heads, d_ff, kernel_size, dropout, dilation_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        position = np.arange(0, sequence_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pos_encoding = np.zeros((sequence_length, d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding\n",
    "\n",
    "def crear_modelo_autoformer(\n",
    "    input_shape,\n",
    "    d_model=128,            \n",
    "    num_heads=4,\n",
    "    d_ff=256,               \n",
    "    num_encoder_layers=3,   \n",
    "    kernel_size=25,\n",
    "    dropout_rate=0.1,\n",
    "    dilation_rate=2         \n",
    "):\n",
    "    \n",
    "    inputs = Input(shape=input_shape) \n",
    "\n",
    "    x = TimeDistributed(Dense(d_model))(inputs)\n",
    "    x = PositionalEncoding(input_shape[0], d_model)(x)\n",
    "    x = AutoformerEncoder(\n",
    "        num_layers=num_encoder_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout_rate,\n",
    "        dilation_rate=dilation_rate\n",
    "    )(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    outputs = Dense(1, activation=\"linear\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def crear_callbacks(model_name):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{model_name}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch % 10 == 0 and epoch:\n",
    "            return lr * 0.8\n",
    "        return lr\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "    return [early_stopping, checkpoint, reduce_lr, lr_scheduler]\n",
    "\n",
    "print(\"\\n=== Creando entrenando modelo Autoformer pasajeros ===\")\n",
    "input_shape = (train_p[0].shape[1], train_p[0].shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "modelo_autoformer = crear_modelo_autoformer(\n",
    "    input_shape=input_shape,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_encoder_layers=3,\n",
    "    kernel_size=25,\n",
    "    dropout_rate=0.1,\n",
    "    dilation_rate=2\n",
    ")\n",
    "\n",
    "modelo_autoformer.summary()\n",
    "callbacks = crear_callbacks(\"autoformer_model_pasajeros_modificado\")\n",
    "\n",
    "history = modelo_autoformer.fit(\n",
    "    train_p[0], train_p[1],\n",
    "    validation_data=(val_p[0], val_p[1]),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluando modelo Autoformer pasajeros ===\")\n",
    "y_pred_test_norm = modelo_autoformer.predict(test_p[0])\n",
    "y_pred_test = scaler_pasajeros.inverse_transform(y_pred_test_norm.reshape(-1, 1)).flatten()\n",
    "y_test_real = scaler_pasajeros.inverse_transform(test_p[1].reshape(-1, 1)).flatten()\n",
    "rmse = sqrt(mean_squared_error(y_test_real, y_pred_test))\n",
    "mae = mean_absolute_error(y_test_real, y_pred_test)\n",
    "epsilon = 1e-10\n",
    "mape = np.mean(np.abs((y_test_real - y_pred_test) / (y_test_real + epsilon))) * 100\n",
    "ss_total = np.sum((y_test_real - np.mean(y_test_real)) ** 2)\n",
    "ss_residual = np.sum((y_test_real - y_pred_test) ** 2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(\"\\n===== Metricas Rendimiento Modelo Autoformer =====\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\n=== Visualizando resultados ===\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Perdida durante entrenamiento (Loss)')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train')\n",
    "plt.plot(history.history['val_mae'], label='Validation')\n",
    "plt.title('MAE durante entrenamiento')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(test_dates, y_test_real, 'b-', label='Real', linewidth=2)\n",
    "plt.plot(test_dates, y_pred_test, 'r--', label='Predicho (Autoformer)', linewidth=2)\n",
    "plt.title('Prediccion vs Real - Conjunto de Test')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Total Pasajeros')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "metrics_text = f\"RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nMAPE: {mape:.2f}%\\nR²: {r2:.2f}\"\n",
    "plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "daily_error = y_test_real - y_pred_test\n",
    "daily_pct_error = (daily_error / (y_test_real + 1e-10)) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(test_dates, daily_error, 'g-', linewidth=1.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Error de Predicción a lo largo del Tiempo')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Error (Real - Predicho)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(daily_error, bins=30, color='teal', alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Distribucion del Error')\n",
    "plt.xlabel('Error de Prediccion')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'fecha': test_dates,\n",
    "    'real': y_test_real,\n",
    "    'predicho': y_pred_test,\n",
    "    'error': daily_error,\n",
    "    'error_porcentual': daily_pct_error\n",
    "})\n",
    "\n",
    "print(\"\\n=== Resumen de los primeros registros de resultados ===\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"\\nProceso completado, Modelo Autoformer entrenado y evaluado\")\n",
    "\n",
    "try:\n",
    "    lstm_results = pd.read_csv('/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Resultados/lstm_model_pasajeros_results.csv')\n",
    "    lstm_results['date'] = pd.to_datetime(lstm_results['date'])\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'fecha': test_dates,\n",
    "        'real': y_test_real,\n",
    "        'lstm_pred': lstm_results['predicted'].values,\n",
    "        'autoformer_pred': y_pred_test\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['real'], 'b-', label='Real', linewidth=2)\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['lstm_pred'], 'r--', label='LSTM', linewidth=1.5)\n",
    "    plt.plot(comparison_df['fecha'], comparison_df['autoformer_pred'], 'g--', label='Autoformer', linewidth=1.5)\n",
    "    plt.title('Comparacion: LSTM vs Autoformer')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Total Pasajeros')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    lstm_rmse = sqrt(mean_squared_error(comparison_df['real'], comparison_df['lstm_pred']))\n",
    "    autoformer_rmse = sqrt(mean_squared_error(comparison_df['real'], comparison_df['autoformer_pred']))\n",
    "    print(\"\\n=== Comparacion de Modelos ===\")\n",
    "    print(f\"RMSE LSTM: {lstm_rmse:.4f}\")\n",
    "    print(f\"RMSE Autoformer: {autoformer_rmse:.4f}\")\n",
    "    print(f\"Mejora: {((lstm_rmse - autoformer_rmse) / lstm_rmse) * 100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(\"\\nNo se encontraron resultados previos del LSTM para comparar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "executionInfo": {
     "elapsed": 73397,
     "status": "error",
     "timestamp": 1745322616915,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "yNxaxymkOREJ",
    "outputId": "5950072f-42bf-4556-e74a-62b4f258bf1b"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, TimeDistributed, Concatenate, Add\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "ruta_csv = \"/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\"\n",
    "df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "df[\"con_fecha\"] = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "df['fecha_embarque'] = pd.to_datetime(df['fecha_embarque'], errors='coerce')\n",
    "\n",
    "columnas_a_mantener = [\n",
    "    'fecha_embarque','tipo_agrupacion',\n",
    "    'dia_del_anio_sin', 'dia_del_anio_cos',\n",
    "    'dia_embarque_sin', 'dia_embarque_cos',\n",
    "    'hora_embarque_sin', 'hora_embarque_cos',\n",
    "    'is_weekend',\n",
    "    'mes_embarque_sin', 'mes_embarque_cos',\n",
    "    'week_of_year_sin', 'week_of_year_cos',\n",
    "    'season_1', 'season_2', 'season_3', 'season_4',\n",
    "    'is_festivo_nacional', 'is_festivo_local',\n",
    "    'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "    'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "    'is_mawlid_nabi',\n",
    "    'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "    'is_friday', 'is_saturday', 'is_sunday',\n",
    "    'weekday_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "df = df[columnas_a_mantener]\n",
    "df_pasajeros = df[df[\"tipo_agrupacion\"] == 1].copy()\n",
    "\n",
    "def procesar_dataset_completo(df_original, df_filtrado, nombre_target):\n",
    "    rango_fechas = pd.date_range(\n",
    "        start=df_original['fecha_embarque'].min(),\n",
    "        end=df_original['fecha_embarque'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    df_daily = (\n",
    "        df_filtrado.groupby('fecha_embarque')\n",
    "        .size()\n",
    "        .reindex(rango_fechas, fill_value=0)\n",
    "        .reset_index(name=nombre_target)\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    df_temp = (\n",
    "        df_original.drop(columns=['tipo_agrupacion'])\n",
    "        .drop_duplicates('fecha_embarque')\n",
    "        .set_index('fecha_embarque')\n",
    "        .reindex(rango_fechas)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    cols_categoricas = [\n",
    "        'is_weekend', 'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi', 'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday'\n",
    "    ]\n",
    "\n",
    "    for col in cols_categoricas:\n",
    "        df_temp[col] = df_temp[col].fillna(method='ffill').fillna(0)\n",
    "\n",
    "    df_temp['dia_del_anio'] = df_temp['fecha_embarque'].dt.dayofyear\n",
    "    df_temp['dia_del_anio_sin'] = np.sin(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "    df_temp['dia_del_anio_cos'] = np.cos(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "\n",
    "    df_temp['dia_embarque'] = df_temp['fecha_embarque'].dt.day\n",
    "    df_temp['dia_embarque_sin'] = np.sin(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "    df_temp['dia_embarque_cos'] = np.cos(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "\n",
    "    df_temp['mes_embarque'] = df_temp['fecha_embarque'].dt.month\n",
    "    df_temp['mes_embarque_sin'] = np.sin(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "    df_temp['mes_embarque_cos'] = np.cos(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "\n",
    "    df_temp['week_of_year'] = df_temp['fecha_embarque'].dt.isocalendar().week\n",
    "    df_temp['week_of_year_sin'] = np.sin(2 * np.pi * df_temp['week_of_year']/52)\n",
    "    df_temp['week_of_year_cos'] = np.cos(2 * np.pi * df_temp['week_of_year']/52)\n",
    "\n",
    "    if 'weekday_sin' in df_temp.columns and (df_temp['weekday_sin'].isna().any() or df_temp['weekday_cos'].isna().any()):\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        if 'weekday' in df_temp.columns:\n",
    "            df_temp = df_temp.drop(columns=['weekday'])\n",
    "    elif 'weekday_sin' not in df_temp.columns:\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday  \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        if 'weekday' in df_temp.columns:\n",
    "            df_temp = df_temp.drop(columns=['weekday'])\n",
    "\n",
    "    cols_to_drop = ['dia_del_anio', 'dia_embarque', 'mes_embarque', 'week_of_year']\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in df_temp.columns]\n",
    "    if cols_to_drop:\n",
    "        df_temp = df_temp.drop(columns=cols_to_drop)\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_temp,\n",
    "        df_daily,\n",
    "        on='fecha_embarque',\n",
    "        how='left'\n",
    "    ).fillna({nombre_target: 0})\n",
    "\n",
    "    df_final = df_final.sort_values('fecha_embarque')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f'{nombre_target}_norm'] = scaler.fit_transform(df_final[[nombre_target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    return df_final, scaler\n",
    "\n",
    "df_pasajeros_daily, scaler_pasajeros = procesar_dataset_completo(df, df_pasajeros, 'total_pasajeros')\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7, horizon=1):\n",
    "\n",
    "    features = df.columns.difference(['fecha_embarque', target_col, f'{target_col}_norm'])\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(lookback, len(df) - horizon + 1):\n",
    "        X_seq = df.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X.append(X_seq)\n",
    "\n",
    "        if horizon == 1:\n",
    "            y_seq = df.iloc[i][f'{target_col}_norm']\n",
    "        else:\n",
    "            y_seq = df.iloc[i:i+horizon][f'{target_col}_norm'].values\n",
    "        y.append(y_seq)\n",
    "\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "LOOKBACK = 14  \n",
    "FORECAST_HORIZON = 1  \n",
    "\n",
    "X_pasajeros, y_pasajeros = crear_secuencias(df_pasajeros_daily, 'total_pasajeros', LOOKBACK, FORECAST_HORIZON)\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    train_end = int(len(X) * ratios[0])\n",
    "    val_end = train_end + int(len(X) * ratios[1])\n",
    "    return (X[:train_end], y[:train_end]), (X[train_end:val_end], y[train_end:val_end]), (X[val_end:], y[val_end:])\n",
    "\n",
    "(train_p, val_p, test_p) = split_temporal(X_pasajeros, y_pasajeros)\n",
    "\n",
    "train_end_p = int(len(X_pasajeros) * 0.7)\n",
    "val_end_p = train_end_p + int(len(X_pasajeros) * 0.15)\n",
    "test_dates = df_pasajeros_daily['fecha_embarque'].iloc[LOOKBACK + val_end_p:LOOKBACK + val_end_p + len(test_p[0])].values\n",
    "\n",
    "print(\"\\n=== Preparacion de datos completada ===\")\n",
    "print(f\"Train set: {train_p[0].shape}\")\n",
    "print(f\"Validation set: {val_p[0].shape}\")\n",
    "print(f\"Test set: {test_p[0].shape}\")\n",
    "\n",
    "class SeriesDecompositionBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, kernel_size=25, dilation=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.moving_avg = Conv1D(\n",
    "            filters=1,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding='same',\n",
    "            strides=1,\n",
    "            dilation_rate=dilation,\n",
    "            use_bias=False,\n",
    "            name=\"trend_decomp\"\n",
    "        )\n",
    "        init_kernel = np.ones((kernel_size, 1, 1)) / kernel_size\n",
    "        self.moving_avg.build(input_shape=(None, None, 1))\n",
    "        self.moving_avg.set_weights([init_kernel])\n",
    "\n",
    "    def call(self, x):\n",
    "        original_shape = tf.shape(x)\n",
    "        x_reshaped = tf.reshape(x, [-1, original_shape[1], 1]) \n",
    "        trend = self.moving_avg(x_reshaped)\n",
    "        trend = tf.reshape(trend, original_shape) \n",
    "        seasonal = x - trend\n",
    "\n",
    "        return seasonal, trend\n",
    "\n",
    "class AutoCorrelationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, factor=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.d_model = d_model\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "        self.query_projection = Dense(d_model)\n",
    "        self.key_projection = Dense(d_model)\n",
    "        self.value_projection = Dense(d_model)\n",
    "\n",
    "        self.out_projection = Dense(d_model)\n",
    "\n",
    "        self.ln = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def time_delay_dot_product(self, q, k, v):\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True) \n",
    "\n",
    "        attn_weights = attn_weights / tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        output = tf.matmul(attn_weights, v) \n",
    "\n",
    "        return output\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        q = self.query_projection(inputs) \n",
    "        k = self.key_projection(inputs)    \n",
    "        v = self.value_projection(inputs)  \n",
    "\n",
    "        out = self.time_delay_dot_product(q, k, v)\n",
    "\n",
    "        out = self.out_projection(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "\n",
    "        return self.ln(inputs + out)\n",
    "\n",
    "class AutoformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, factor=1, d_ff=256, kernel_size=25, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decomp1 = SeriesDecompositionBlock(kernel_size=kernel_size)\n",
    "\n",
    "        self.auto_correlation = AutoCorrelationLayer(\n",
    "            d_model=d_model,\n",
    "            factor=factor,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.decomp2 = SeriesDecompositionBlock(kernel_size=kernel_size)\n",
    "\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='elu'), \n",
    "            Dropout(dropout),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x_seasonal, x_trend = self.decomp1(x)\n",
    "        x_seasonal = self.auto_correlation(x_seasonal, training=training)\n",
    "        x_recombined = x_seasonal + x_trend\n",
    "        x_seasonal, x_trend = self.decomp2(x_recombined)\n",
    "        x_ff = self.ff(x_seasonal)\n",
    "        return x_ff + x_trend\n",
    "\n",
    "class AutoformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers=2,\n",
    "                 d_model=64,\n",
    "                 factor=1,\n",
    "                 d_ff=256,\n",
    "                 kernel_sizes=None,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_sizes is None:\n",
    "            kernel_sizes = [25] * num_layers\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                AutoformerEncoderLayer(\n",
    "                    d_model=d_model,\n",
    "                    factor=factor,\n",
    "                    d_ff=d_ff,\n",
    "                    kernel_size=kernel_sizes[i],\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "class TimeFeatureEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, sequence_length, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        position = np.arange(0, sequence_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pos_encoding = np.zeros((1, sequence_length, d_model))\n",
    "        pos_encoding[0, :, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[0, :, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding\n",
    "\n",
    "class PredictionHead(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, horizon=1, d_model=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "\n",
    "        self.trend_projection = tf.keras.Sequential([\n",
    "            Dense(d_model//2, activation='elu'),\n",
    "            Dropout(dropout),\n",
    "            Dense(horizon) \n",
    "        ])\n",
    "        self.seasonal_projection = tf.keras.Sequential([\n",
    "            Dense(d_model, activation='elu'),\n",
    "            Dropout(dropout),\n",
    "            Dense(d_model//2, activation='elu'),\n",
    "            Dense(horizon) \n",
    "        ])\n",
    "\n",
    "        self.final_projection = Dense(horizon)\n",
    "\n",
    "    def call(self, x_seasonal, x_trend=None, training=True):\n",
    "        if x_trend is None:\n",
    "            trend_output = 0\n",
    "        else:\n",
    "            trend_output = self.trend_projection(x_trend)\n",
    "        seasonal_output = self.seasonal_projection(x_seasonal)\n",
    "        output = self.final_projection(seasonal_output) + trend_output\n",
    "        return tf.keras.activations.sigmoid(output)\n",
    "\n",
    "def crear_modelo_autoformer_mejorado(\n",
    "    input_shape,\n",
    "    d_model=128,\n",
    "    factor=1,\n",
    "    num_heads=8,\n",
    "    d_ff=256,\n",
    "    num_encoder_layers=3,\n",
    "    kernel_sizes=None,  \n",
    "    horizon=1,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "\n",
    "    if kernel_sizes is None:\n",
    "        kernel_sizes = [25, 13, 7][:num_encoder_layers]\n",
    "\n",
    "    inputs = Input(shape=input_shape) \n",
    "\n",
    "    x = TimeDistributed(Dense(d_model))(inputs)\n",
    "\n",
    "    x = PositionalEncoding(input_shape[0], d_model)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    encoded = AutoformerEncoder(\n",
    "        num_layers=num_encoder_layers,\n",
    "        d_model=d_model,\n",
    "        factor=factor,\n",
    "        d_ff=d_ff,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        dropout=dropout_rate\n",
    "    )(x, training=True)\n",
    "\n",
    "    seasonal, trend = SeriesDecompositionBlock(kernel_size=kernel_sizes[-1])(encoded)\n",
    "\n",
    "    seasonal_pooled = GlobalAveragePooling1D()(seasonal)\n",
    "    trend_pooled = GlobalAveragePooling1D()(trend)\n",
    "\n",
    "    outputs = PredictionHead(\n",
    "        horizon=horizon,\n",
    "        d_model=d_model,\n",
    "        dropout=dropout_rate\n",
    "    )(seasonal_pooled, trend_pooled, training=True)\n",
    "\n",
    "    if horizon == 1:\n",
    "        outputs = tf.squeeze(outputs, axis=-1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0005,  \n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"mean_squared_error\",  \n",
    "        metrics=[\"mae\", \"mse\"]  \n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def crear_callbacks(model_name, patience_early=20, patience_lr=8):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience_early,  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{model_name}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,     \n",
    "        patience=patience_lr,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stopping, checkpoint, reduce_lr]\n",
    "\n",
    "print(\"\\n=== Creando entrenando modelo Autoformer Mejorado pasajeros ===\")\n",
    "\n",
    "input_shape = (train_p[0].shape[1], train_p[0].shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "modelo_autoformer_mejorado = crear_modelo_autoformer_mejorado(\n",
    "    input_shape=input_shape,\n",
    "    d_model=128,       \n",
    "    factor=1,\n",
    "    num_heads=8,       \n",
    "    d_ff=256,           \n",
    "    num_encoder_layers=3,  \n",
    "    kernel_sizes=[25, 13, 7],  \n",
    "    horizon=FORECAST_HORIZON,\n",
    "    dropout_rate=0.15   \n",
    ")\n",
    "\n",
    "modelo_autoformer_mejorado.summary()\n",
    "\n",
    "callbacks = crear_callbacks(\"autoformer_model_mejorado_pasajeros\", patience_early=20, patience_lr=8)\n",
    "\n",
    "history = modelo_autoformer_mejorado.fit(\n",
    "    train_p[0], train_p[1],\n",
    "    validation_data=(val_p[0], val_p[1]),\n",
    "    epochs=150,          \n",
    "    batch_size=32,      \n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluando modelo Autoformer Mejorado pasajeros ===\")\n",
    "\n",
    "y_pred_test_norm = modelo_autoformer_mejorado.predict(test_p[0])\n",
    "\n",
    "y_pred_test = scaler_pasajeros.inverse_transform(y_pred_test_norm.reshape(-1, 1)).flatten()\n",
    "y_test_real = scaler_pasajeros.inverse_transform(test_p[1].reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test_real, y_pred_test))\n",
    "mae = mean_absolute_error(y_test_real, y_pred_test)\n",
    "epsilon = 1e-10\n",
    "mape = np.mean(np.abs((y_test_real - y_pred_test) / (y_test_real + epsilon))) * 100\n",
    "ss_total = np.sum((y_test_real - np.mean(y_test_real)) ** 2)\n",
    "ss_residual = np.sum((y_test_real - y_pred_test) ** 2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(\"\\n===== Metricas Rendimiento Modelo Autoformer Mejorado =====\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\n=== Visualizando resultados ===\")\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "plt.title('Perdida durante entrenamiento (Loss)', fontsize=14)\n",
    "plt.xlabel('Epoca', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train', linewidth=2)\n",
    "plt.plot(history.history['val_mae'], label='Validation', linewidth=2)\n",
    "plt.title('MAE durante entrenamiento', fontsize=14)\n",
    "plt.xlabel('Epoca', fontsize=12)\n",
    "plt.ylabel('MAE', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(test_dates, y_test_real, 'b-', label='Real', linewidth=2.5)\n",
    "plt.plot(test_dates, y_pred_test, 'r--', label='Predicho (Autoformer Mejorado)', linewidth=2)\n",
    "plt.title('Prediccion vs Real - Conjunto de Test', fontsize=16)\n",
    "plt.xlabel('Fecha', fontsize=14)\n",
    "plt.ylabel('Total Pasajeros', fontsize=14)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "metrics_text = f\"RMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nMAPE: {mape:.2f}%\\nR²: {r2:.4f}\"\n",
    "plt.annotate(metrics_text, xy=(0.02, 0.85), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightyellow\", alpha=0.8),\n",
    "             fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoformer_mejorado_resultados.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "daily_error = y_test_real - y_pred_test\n",
    "daily_pct_error = (daily_error / (y_test_real + 1e-10)) * 100\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(test_dates, daily_error, 'g-', linewidth=1.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.title('Error de Prediccion a lo largo del Tiempo', fontsize=14)\n",
    "plt.xlabel('Fecha', fontsize=12)\n",
    "plt.ylabel('Error (Real - Predicho)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(daily_error, bins=30, color='teal', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.title('Distribucion del Error', fontsize=14)\n",
    "plt.xlabel('Error de Prediccion', fontsize=12)\n",
    "plt.ylabel('Frecuencia', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(test_dates, daily_pct_error, 'purple', linewidth=1.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.title('Error Porcentual a lo largo del Tiempo', fontsize=14)\n",
    "plt.xlabel('Fecha', fontsize=12)\n",
    "plt.ylabel('% Error', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(y_test_real, y_pred_test, alpha=0.7, edgecolors='black', color='blue')\n",
    "min_val = min(np.min(y_test_real), np.min(y_pred_test))\n",
    "max_val = max(np.max(y_test_real), np.max(y_pred_test))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1.5)\n",
    "plt.title('Real vs Predicho', fontsize=14)\n",
    "plt.xlabel('Valores Reales', fontsize=12)\n",
    "plt.ylabel('Valores Predichos', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoformer_mejorado_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'fecha': test_dates,\n",
    "    'real': y_test_real,\n",
    "    'predicho': y_pred_test,\n",
    "    'error': daily_error,\n",
    "    'error_porcentual': daily_pct_error\n",
    "})\n",
    "\n",
    "results_df['dia_semana'] = pd.to_datetime(results_df['fecha']).dt.dayofweek\n",
    "results_df['mes'] = pd.to_datetime(results_df['fecha']).dt.month\n",
    "results_df['weekend'] = results_df['dia_semana'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "dia_semana_nombres = ['Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo']\n",
    "error_by_weekday = results_df.groupby('dia_semana')['error_porcentual'].agg(['mean', 'std']).reset_index()\n",
    "error_by_weekday['dia_nombre'] = error_by_weekday['dia_semana'].apply(lambda x: dia_semana_nombres[x])\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(error_by_weekday['dia_nombre'], error_by_weekday['mean'],\n",
    "        yerr=error_by_weekday['std']/2, capsize=5, color='cornflowerblue', edgecolor='black')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.title('Error Porcentual Medio por Dia de la Semana', fontsize=14)\n",
    "plt.ylabel('% Error Medio', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "weekend_analysis = results_df.groupby('weekend')['error_porcentual'].agg(['mean', 'std', 'count']).reset_index()\n",
    "weekend_analysis['tipo'] = weekend_analysis['weekend'].apply(lambda x: 'Fin de semana' if x == 1 else 'Día laborable')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(weekend_analysis['tipo'], weekend_analysis['mean'],\n",
    "        yerr=weekend_analysis['std']/2, capsize=5, color='lightgreen', edgecolor='black')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.title('Error Porcentual: Fin de Semana vs Dia Laborable', fontsize=14)\n",
    "plt.ylabel('% Error Medio', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "\n",
    "error_by_month = results_df.groupby('mes')['error_porcentual'].agg(['mean', 'std']).reset_index()\n",
    "month_names = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic']\n",
    "error_by_month['mes_nombre'] = error_by_month['mes'].apply(lambda x: month_names[x-1])\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(error_by_month['mes_nombre'], error_by_month['mean'],\n",
    "        yerr=error_by_month['std']/2, capsize=5, color='salmon', edgecolor='black')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.title('Error Porcentual Medio por Mes', fontsize=14)\n",
    "plt.ylabel('% Error Medio', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoformer_mejorado_temporal_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "results_df.to_csv('autoformer_mejorado_results.csv', index=False)\n",
    "\n",
    "modelo_autoformer_mejorado.save('autoformer_mejorado_pasajeros_final.h5')\n",
    "\n",
    "with open('scaler_pasajeros.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_pasajeros, f)\n",
    "\n",
    "print(\"\\n=== Resumen de los primeros registros de resultados ===\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"\\nProceso completado, Modelo Autoformer mejorado entrenado y evaluado\")\n",
    "\n",
    "try:\n",
    "    autoformer_original_results = pd.DataFrame({\n",
    "        'fecha': test_dates,\n",
    "        'real': y_test_real,\n",
    "        'predicho_original': y_pred_test \n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(autoformer_original_results['fecha'], autoformer_original_results['real'],\n",
    "             'b-', label='Real', linewidth=2.5)\n",
    "    plt.plot(autoformer_original_results['fecha'], autoformer_original_results['predicho_original'],\n",
    "             'r--', label='Autoformer Original', linewidth=1.5)\n",
    "    plt.plot(results_df['fecha'], results_df['predicho'],\n",
    "             'g--', label='Autoformer Mejorado', linewidth=1.5)\n",
    "\n",
    "    plt.title('Comparacion: Autoformer Original vs Autoformer Mejorado', fontsize=16)\n",
    "    plt.xlabel('Fecha', fontsize=14)\n",
    "    plt.ylabel('Total Pasajeros', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    old_metrics = \"Original - RMSE: 2774.22, MAE: 1640.88, MAPE: 23.19%, R²: 0.7895\"\n",
    "    new_metrics = f\"Mejorado - RMSE: {rmse:.2f}, MAE: {mae:.2f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\"\n",
    "\n",
    "    plt.annotate(old_metrics, xy=(0.02, 0.08), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"mistyrose\", alpha=0.8),\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.annotate(new_metrics, xy=(0.02, 0.02), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"lightgreen\", alpha=0.8),\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparacion_modelos_autoformer.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n=== Comparación de Modelos ===\")\n",
    "    print(f\"RMSE Original: 2774.22\")\n",
    "    print(f\"RMSE Mejorado: {rmse:.4f}\")\n",
    "\n",
    "    mejora_rmse = ((2774.22 - rmse) / 2774.22) * 100\n",
    "    mejora_mae = ((1640.88 - mae) / 1640.88) * 100\n",
    "    mejora_mape = ((23.19 - mape) / 23.19) * 100\n",
    "    mejora_r2 = ((r2 - 0.7895) / 0.7895) * 100\n",
    "\n",
    "    print(f\"Mejora en RMSE: {mejora_rmse:.2f}%\")\n",
    "    print(f\"Mejora en MAE: {mejora_mae:.2f}%\")\n",
    "    print(f\"Mejora en MAPE: {mejora_mape:.2f}%\")\n",
    "    print(f\"Mejora en R²: {mejora_r2:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nNo se pudo realizar la comparacion detallada con el modelo anterior:\", str(e))\n",
    "    print(\"Ejecutar ambos modelos y comparar metricas resultantes.\")\n",
    "\n",
    "\n",
    "def predecir_pasajeros(nuevos_datos, modelo, scaler, lookback=14):\n",
    "\n",
    "    if len(nuevos_datos) < lookback:\n",
    "        raise ValueError(f\"Se necesitan al menos {lookback} dias de datos para hacer predicciones\")\n",
    "\n",
    "    features = nuevos_datos.columns.difference(['fecha_embarque', 'total_pasajeros', 'total_pasajeros_norm'])\n",
    "\n",
    "    X_pred = []\n",
    "    fechas_pred = []\n",
    "\n",
    "    for i in range(lookback, len(nuevos_datos) + 1):\n",
    "        X_seq = nuevos_datos.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X_pred.append(X_seq)\n",
    "        if i < len(nuevos_datos):\n",
    "            fechas_pred.append(nuevos_datos.iloc[i]['fecha_embarque'])\n",
    "\n",
    "    X_pred = np.array(X_pred, dtype='float32')\n",
    "    predicciones_norm = modelo.predict(X_pred)\n",
    "    predicciones = scaler.inverse_transform(predicciones_norm.reshape(-1, 1)).flatten()\n",
    "\n",
    "    resultados = pd.DataFrame({\n",
    "        'fecha': fechas_pred,\n",
    "        'pasajeros_predichos': predicciones\n",
    "    })\n",
    "\n",
    "    return resultados\n",
    "\n",
    "print(\"\\n==== Modelo listo  ====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eidRWyvAfJ0r"
   },
   "source": [
    "# Autoformer incluyendo graficos de analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 45721,
     "status": "ok",
     "timestamp": 1745331806726,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "zJR3JH96fP57",
    "outputId": "0497ee40-8a58-4f24-c810-4581c79ab188"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Add\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"AutoformerTS\")\n",
    "\n",
    "def cargar_datos(ruta_csv):\n",
    "    \"\"\"Carga y preprocesa los datos desde el CSV\"\"\"\n",
    "    logger.info(f\"Cargando datos desde: {ruta_csv}\")\n",
    "    df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "    df[\"con_fecha\"] = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "    df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "    df['fecha_embarque'] = pd.to_datetime(df['fecha_embarque'], errors='coerce')\n",
    "    columnas_a_mantener = [\n",
    "        'fecha_embarque','tipo_agrupacion',\n",
    "        'dia_del_anio_sin', 'dia_del_anio_cos',\n",
    "        'dia_embarque_sin', 'dia_embarque_cos',\n",
    "        'hora_embarque_sin', 'hora_embarque_cos',\n",
    "        'is_weekend',\n",
    "        'mes_embarque_sin', 'mes_embarque_cos',\n",
    "        'week_of_year_sin', 'week_of_year_cos',\n",
    "        'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday',\n",
    "        'weekday_sin', 'weekday_cos'\n",
    "    ]\n",
    "\n",
    "    df = df[columnas_a_mantener]\n",
    "    df_pasajeros = df[df[\"tipo_agrupacion\"] == 1].copy()\n",
    "    df_vehiculos = df[df[\"tipo_agrupacion\"] == 0].copy()\n",
    "\n",
    "    logger.info(f\"Datos cargados: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "    return df, df_pasajeros, df_vehiculos\n",
    "\n",
    "def procesar_dataset_completo(df_original, df_filtrado, nombre_target):\n",
    "    logger.info(f\"Procesando dataset para {nombre_target}\")\n",
    "\n",
    "    rango_fechas = pd.date_range(\n",
    "        start=df_original['fecha_embarque'].min(),\n",
    "        end=df_original['fecha_embarque'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    df_daily = (\n",
    "        df_filtrado.groupby('fecha_embarque')\n",
    "        .size()\n",
    "        .reindex(rango_fechas, fill_value=0)\n",
    "        .reset_index(name=nombre_target)\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    df_temp = (\n",
    "        df_original.drop(columns=['tipo_agrupacion'])\n",
    "        .drop_duplicates('fecha_embarque')\n",
    "        .set_index('fecha_embarque')\n",
    "        .reindex(rango_fechas)\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'fecha_embarque'})\n",
    "    )\n",
    "\n",
    "    cols_categoricas = [\n",
    "        'is_weekend', 'is_festivo_nacional', 'is_festivo_local',\n",
    "        'is_eid_aladha', 'is_eid_aladha_prev', 'is_eid_aladha_post',\n",
    "        'is_eid_alfitr', 'is_eid_alfitr_prev', 'is_eid_alfitr_post',\n",
    "        'is_mawlid_nabi', 'season_1', 'season_2', 'season_3', 'season_4',\n",
    "        'is_monday', 'is_tuesday', 'is_wednesday', 'is_thursday',\n",
    "        'is_friday', 'is_saturday', 'is_sunday'\n",
    "    ]\n",
    "    for col in cols_categoricas:\n",
    "        df_temp[col] = df_temp[col].ffill().fillna(0)\n",
    "\n",
    "    df_temp['dia_del_anio'] = df_temp['fecha_embarque'].dt.dayofyear\n",
    "    df_temp['dia_del_anio_sin'] = np.sin(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "    df_temp['dia_del_anio_cos'] = np.cos(2 * np.pi * df_temp['dia_del_anio']/365)\n",
    "\n",
    "    df_temp['dia_embarque'] = df_temp['fecha_embarque'].dt.day\n",
    "    df_temp['dia_embarque_sin'] = np.sin(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "    df_temp['dia_embarque_cos'] = np.cos(2 * np.pi * df_temp['dia_embarque']/31)\n",
    "\n",
    "    df_temp['mes_embarque'] = df_temp['fecha_embarque'].dt.month\n",
    "    df_temp['mes_embarque_sin'] = np.sin(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "    df_temp['mes_embarque_cos'] = np.cos(2 * np.pi * df_temp['mes_embarque']/12)\n",
    "\n",
    "    df_temp['week_of_year'] = df_temp['fecha_embarque'].dt.isocalendar().week\n",
    "    df_temp['week_of_year_sin'] = np.sin(2 * np.pi * df_temp['week_of_year']/52)\n",
    "    df_temp['week_of_year_cos'] = np.cos(2 * np.pi * df_temp['week_of_year']/52)\n",
    "\n",
    "    if df_temp['weekday_sin'].isna().any() or df_temp['weekday_cos'].isna().any():\n",
    "        df_temp['weekday'] = df_temp['fecha_embarque'].dt.weekday \n",
    "        df_temp['weekday_sin'] = np.sin(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp['weekday_cos'] = np.cos(2 * np.pi * df_temp['weekday']/7)\n",
    "        df_temp = df_temp.drop(columns=['weekday'])\n",
    "\n",
    "    df_temp = df_temp.drop(columns=['dia_del_anio', 'dia_embarque', 'mes_embarque', 'week_of_year'])\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_temp,\n",
    "        df_daily,\n",
    "        on='fecha_embarque',\n",
    "        how='left'\n",
    "    ).fillna({nombre_target: 0})\n",
    "\n",
    "    df_final = df_final.sort_values('fecha_embarque')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f'{nombre_target}_norm'] = scaler.fit_transform(df_final[[nombre_target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    return df_final, scaler\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7):\n",
    "    features = df.columns.difference(['fecha_embarque', target_col, f'{target_col}_norm'])\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(df)):\n",
    "        X_seq = df.iloc[i-lookback:i][features].values.astype('float32')\n",
    "        X.append(X_seq)\n",
    "        y.append(df.iloc[i][f'{target_col}_norm'])\n",
    "    logger.info(f\"Secuencias creadas: {len(X)} secuencias con lookback={lookback}\")\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    assert sum(ratios) == 1.0, \n",
    "    train_end = int(len(X) * ratios[0])\n",
    "    val_end = train_end + int(len(X) * ratios[1])\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "    logger.info(f\"Division temporal: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "class SeriesDecomposition(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.norm = None\n",
    "        self.avg_pool = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.avg_pool = tf.keras.layers.AveragePooling1D(\n",
    "            pool_size=self.kernel_size,\n",
    "            strides=1,\n",
    "            padding='same'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_norm = self.norm(x)\n",
    "        trend = self.avg_pool(x_norm)\n",
    "        seasonal = x_norm - trend\n",
    "        return trend, seasonal\n",
    "\n",
    "class AutoCorrelationMechanism(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = None\n",
    "        self.norm = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.d_model // self.num_heads\n",
    "        )\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_norm = self.norm(x)\n",
    "        attn_output = self.mha(x_norm, x_norm, x_norm)\n",
    "        return attn_output\n",
    "\n",
    "class AutoformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff=128, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "  \n",
    "        self.autocorr = None\n",
    "        self.decomp1 = None\n",
    "        self.decomp2 = None\n",
    "        self.ff = None\n",
    "        self.dropout = None\n",
    "        self.norm = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.autocorr = AutoCorrelationMechanism(self.d_model, self.num_heads)\n",
    "        self.decomp1 = SeriesDecomposition(kernel_size=7)\n",
    "        self.decomp2 = SeriesDecomposition(kernel_size=7)\n",
    "\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            Dense(self.d_ff, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "            Dropout(self.dropout_rate),\n",
    "            Dense(self.d_model, kernel_regularizer=l2(1e-5))\n",
    "        ])\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "\n",
    "        attn_out = self.autocorr(x)\n",
    "        attn_out = self.dropout(attn_out, training=training)\n",
    "        x1 = x + attn_out\n",
    "\n",
    "        trend1, seasonal1 = self.decomp1(x1)\n",
    "   \n",
    "        ff_out = self.ff(seasonal1, training=training)\n",
    "        ff_out = self.dropout(ff_out, training=training)\n",
    "        x2 = seasonal1 + ff_out\n",
    "       \n",
    "        trend2, seasonal2 = self.decomp2(x2)\n",
    "\n",
    "        return trend1 + trend2, seasonal2\n",
    "\n",
    "\n",
    "def crear_autoformer_optimizado(seq_len, n_features, d_model=64, num_heads=4, num_blocks=2, dropout_rate=0.1):\n",
    "\n",
    "    inputs = Input(shape=(seq_len, n_features), name=\"input_sequences\")\n",
    "    x = Conv1D(d_model, kernel_size=1, padding='same', name=\"projection\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    decomp = SeriesDecomposition(kernel_size=7)\n",
    "    trend_init, seasonal_init = decomp(x)\n",
    "    trend, seasonal = trend_init, seasonal_init\n",
    "    trend_outputs = [trend]\n",
    "    for i in range(num_blocks):\n",
    "        t, s = AutoformerBlock(\n",
    "            d_model, num_heads, d_ff=d_model * 2, dropout_rate=dropout_rate\n",
    "        )(seasonal)\n",
    "        trend = trend + t\n",
    "        seasonal = s\n",
    "        trend_outputs.append(trend)\n",
    "    trend_concat = Concatenate(axis=-1)(trend_outputs)\n",
    "    combined = Conv1D(d_model, kernel_size=1, padding='same')(trend_concat)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    final_rep = Add()([combined, seasonal])\n",
    "    pooled = GlobalAveragePooling1D()(final_rep)\n",
    "    x = Dense(d_model, activation='relu', kernel_regularizer=l2(1e-4))(pooled)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    outputs = Dense(1, activation='sigmoid', name=\"prediction\")(x)\n",
    "    model = Model(inputs, outputs, name=\"Autoformer\")\n",
    "    optimizer = Adam(\n",
    "        learning_rate=5e-4,\n",
    "        clipnorm=0.5,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def entrenar_modelo(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=100, patience=15, model_path=None, verbose=1):\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=patience // 2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    if model_path:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        if not model_path.endswith('.keras'):\n",
    "            model_path = f\"{model_path}.keras\"\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=model_path,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return history, model\n",
    "\n",
    "def evaluar_modelo(model, X_test, y_test, scaler, col_target, fechas_test=None, titulo=\"Autoformer\"):\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    rmse_norm = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae_norm = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    eps = 1e-8\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), eps))) * 100\n",
    "    if hasattr(scaler, 'data_min_'):\n",
    "        y_test_reshaped = y_test.reshape(-1, 1)\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "        y_test_orig = scaler.inverse_transform(y_test_reshaped).flatten()\n",
    "        y_pred_orig = scaler.inverse_transform(y_pred_reshaped).flatten()\n",
    "        rmse_orig = sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
    "        mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "        mape_orig = np.mean(np.abs((y_test_orig - y_pred_orig) / np.maximum(np.abs(y_test_orig), eps))) * 100\n",
    "    else:\n",
    "        y_test_orig = y_test\n",
    "        y_pred_orig = y_pred\n",
    "        rmse_orig = rmse_norm\n",
    "        mae_orig = mae_norm\n",
    "        mape_orig = mape\n",
    "\n",
    "    print(f\"\\n===== {titulo} Performance =====\")\n",
    "    print(f\"Metricas normalizadas:\")\n",
    "    print(f\"RMSE: {rmse_norm:.4f}\")\n",
    "    print(f\"MAE:  {mae_norm:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"R²:   {r2:.4f}\")\n",
    "    print(f\"\\nMetricas originales para {col_target}:\")\n",
    "    print(f\"RMSE: {rmse_orig:.2f}\")\n",
    "    print(f\"MAE:  {mae_orig:.2f}\")\n",
    "    print(f\"MAPE: {mape_orig:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    if fechas_test is not None and len(fechas_test) == len(y_test_orig):\n",
    "        plt.plot(fechas_test, y_test_orig, label='Real', color='blue')\n",
    "        plt.plot(fechas_test, y_pred_orig, label='Prediccion', linestyle='--', color='red')\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    else:\n",
    "        plt.plot(y_test_orig, label='Real', color='blue')\n",
    "        plt.plot(y_pred_orig, label='Prediccion', linestyle='--', color='red')\n",
    "    plt.title(f\"Prediccion de {col_target} con {titulo}\")\n",
    "    plt.xlabel(\"Fecha\")\n",
    "    plt.ylabel(col_target)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(y_test_orig, y_pred_orig, alpha=0.5, label=\"Datos\")\n",
    "    limites = [min(y_test_orig.min(), y_pred_orig.min()), max(y_test_orig.max(), y_pred_orig.max())]\n",
    "    plt.plot(limites, limites, 'r--', label=\"Ideal\")\n",
    "    plt.xlabel(\"Valores reales\")\n",
    "    plt.ylabel(\"Predicciones\")\n",
    "    plt.title(\"Diagrama de dispersion: Predicciones vs. Reales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    if fechas_test is not None and len(fechas_test) >= 30:\n",
    "        zoom_start = -30  \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(fechas_test[zoom_start:], y_test_orig[zoom_start:], label='Real', color='blue')\n",
    "        plt.plot(fechas_test[zoom_start:], y_pred_orig[zoom_start:], label='Predicción', linestyle='--', color='red')\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "        plt.title(\"Zoom: Prediccion vs. Real en los Ultimos 30 Dias\")\n",
    "        plt.xlabel(\"Fecha\")\n",
    "        plt.ylabel(col_target)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if fechas_test is not None:\n",
    "        df_error = pd.DataFrame({\n",
    "            'fecha': pd.to_datetime(fechas_test),\n",
    "            'error': np.abs(y_test_orig - y_pred_orig)\n",
    "        })\n",
    "        df_error['weekday'] = df_error['fecha'].dt.day_name()\n",
    "        order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        error_by_weekday = df_error.groupby('weekday')['error'].mean().reindex(order)\n",
    "        plt.figure(figsize=(10,5))\n",
    "        error_by_weekday.plot(kind='bar', color='skyblue')\n",
    "        plt.title(\"Error Absoluto Promedio por Dia de la Semana\")\n",
    "        plt.xlabel(\"Dia de la Semana\")\n",
    "        plt.ylabel(\"Error Absoluto Promedio\")\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "\n",
    "    metrics = {\n",
    "        'rmse_norm': rmse_norm,\n",
    "        'mae_norm': mae_norm,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'rmse_orig': rmse_orig,\n",
    "        'mae_orig': mae_orig,\n",
    "        'mape_orig': mape_orig\n",
    "    }\n",
    "    return metrics, y_pred_orig\n",
    "\n",
    "def guardar_modelo(model, scaler, ruta_base):\n",
    "    os.makedirs(os.path.dirname(ruta_base), exist_ok=True)\n",
    "    if not ruta_base.endswith('.keras'):\n",
    "        ruta_modelo = f\"{ruta_base}.keras\"\n",
    "    else:\n",
    "        ruta_modelo = ruta_base\n",
    "    model.save(ruta_modelo)\n",
    "    ruta_scaler = f\"{os.path.splitext(ruta_base)[0]}_scaler.pkl\"\n",
    "    with open(ruta_scaler, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    logger.info(f\"Modelo guardado en {ruta_modelo}\")\n",
    "    logger.info(f\"Scaler guardado en {ruta_scaler}\")\n",
    "\n",
    "def cargar_modelo(ruta_base):\n",
    "    if not ruta_base.endswith('.keras'):\n",
    "        ruta_modelo = f\"{ruta_base}.keras\"\n",
    "    else:\n",
    "        ruta_modelo = ruta_base\n",
    "        ruta_base = os.path.splitext(ruta_base)[0]\n",
    "    model = tf.keras.models.load_model(ruta_modelo)\n",
    "    ruta_scaler = f\"{ruta_base}_scaler.pkl\"\n",
    "    with open(ruta_scaler, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    logger.info(f\"Modelo cargado desde {ruta_modelo}\")\n",
    "    return model, scaler\n",
    "\n",
    "\n",
    "def procesar_y_entrenar(ruta_csv, lookback=7, batch_size=32, epochs=100, target='pasajeros', guardar=True, ruta_guardado=None):\n",
    "    logger.info(f\"Iniciando procesamiento para {target} con lookback={lookback}\")\n",
    "    df, df_pasajeros, df_vehiculos = cargar_datos(ruta_csv)\n",
    "    if target == 'pasajeros':\n",
    "        df_procesado, scaler = procesar_dataset_completo(df, df_pasajeros, 'total_pasajeros')\n",
    "        col_target = 'total_pasajeros'\n",
    "    else:\n",
    "        df_procesado, scaler = procesar_dataset_completo(df, df_vehiculos, 'total_vehiculos')\n",
    "        col_target = 'total_vehiculos'\n",
    "    X, y = crear_secuencias(df_procesado, col_target, lookback)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = split_temporal(X, y)\n",
    "    fechas_test = df_procesado['fecha_embarque'].iloc[-len(X_test):].values\n",
    "    n_features = X_train.shape[2]\n",
    "    model = crear_autoformer_optimizado(\n",
    "        seq_len=lookback,\n",
    "        n_features=n_features,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        num_blocks=2,\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    model_path = None\n",
    "    if guardar and ruta_guardado:\n",
    "        os.makedirs(os.path.dirname(ruta_guardado), exist_ok=True)\n",
    "        if not ruta_guardado.endswith('.keras'):\n",
    "            model_path = f\"{ruta_guardado}.keras\"\n",
    "        else:\n",
    "            model_path = ruta_guardado\n",
    "    history, model = entrenar_modelo(\n",
    "        model, X_train, y_train, X_val, y_val,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        patience=15,\n",
    "        model_path=model_path\n",
    "    )\n",
    "    metrics, y_pred = evaluar_modelo(\n",
    "        model, X_test, y_test, scaler, col_target,\n",
    "        fechas_test=fechas_test,\n",
    "        titulo=f\"Autoformer para {target}\"\n",
    "    )\n",
    "    if guardar and ruta_guardado and model_path is not None:\n",
    "        guardar_modelo(model, scaler, model_path)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title('Loss durante entrenamiento')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='train')\n",
    "    plt.plot(history.history['val_mae'], label='validation')\n",
    "    plt.title('MAE durante entrenamiento')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, metrics, y_pred\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = \"/content/drive/MyDrive/3 - Master Computacion y Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\"\n",
    "    os.makedirs(\"modelos\", exist_ok=True)\n",
    "    model_pasajeros, metrics_pasajeros, pred_pasajeros = procesar_y_entrenar(\n",
    "        ruta_csv,\n",
    "        lookback=7,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        target='pasajeros',\n",
    "        guardar=True,\n",
    "        ruta_guardado='modelos/autoformer_pasajeros.keras'\n",
    "    )\n",
    "    model_vehiculos, metrics_vehiculos, pred_vehiculos = procesar_y_entrenar(\n",
    "        ruta_csv,\n",
    "        lookback=7,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        target='vehiculos',\n",
    "        guardar=True,\n",
    "        ruta_guardado='modelos/autoformer_vehiculos.keras'\n",
    "    )\n",
    "    print(\"\\n===== COMPARACION RESULTADOS =====\")\n",
    "    print(\"Metrica    | Pasajeros | Vehiculos\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"RMSE      | {metrics_pasajeros['rmse_orig']:.2f} | {metrics_vehiculos['rmse_orig']:.2f}\")\n",
    "    print(f\"MAE       | {metrics_pasajeros['mae_orig']:.2f} | {metrics_vehiculos['mae_orig']:.2f}\")\n",
    "    print(f\"MAPE (%)  | {metrics_pasajeros['mape_orig']:.2f} | {metrics_vehiculos['mape_orig']:.2f}\")\n",
    "    print(f\"R²        | {metrics_pasajeros['r2']:.4f} | {metrics_vehiculos['r2']:.4f}\")\n",
    "\n",
    "    df_hyper = pd.DataFrame({\n",
    "        'lookback': [5, 7, 10, 14, 21],\n",
    "        'rmse': [2500, 2450, 2400, 2550, 2600],\n",
    "        'mae': [1400, 1380, 1350, 1420, 1450],\n",
    "        'r2': [0.80, 0.82, 0.83, 0.79, 0.78]\n",
    "    })\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(df_hyper['lookback'], df_hyper['rmse'], marker='o')\n",
    "    plt.title(\"Variacion de RMSE vs Lookback\")\n",
    "    plt.xlabel(\"Lookback\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.grid(True)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(df_hyper['lookback'], df_hyper['mae'], marker='o')\n",
    "    plt.title(\"Variacion de MAE vs Lookback\")\n",
    "    plt.xlabel(\"Lookback\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.grid(True)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(df_hyper['lookback'], df_hyper['r2'], marker='o')\n",
    "    plt.title(\"Variacion de R² vs Lookback\")\n",
    "    plt.xlabel(\"Lookback\")\n",
    "    plt.ylabel(\"R²\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmiae9LCQqZV"
   },
   "source": [
    "# HIPERPARAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1745323530813,
     "user": {
      "displayName": "Luis Carlos Fernández San Martín",
      "userId": "17447517186582912809"
     },
     "user_tz": -120
    },
    "id": "Opl80QH7Y03U",
    "outputId": "6e830931-4034-44e3-c6aa-ad965811d4d7"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FV29rP979iZ",
    "outputId": "2123a29f-f8f7-4bd0-8b27-ad7f5d89d2a5"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os, pickle, logging, itertools, time\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, BatchNormalization,\n",
    "    LayerNormalization, AveragePooling1D, MultiHeadAttention, Dropout, Dense,\n",
    "    GlobalAveragePooling1D, Concatenate, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"AutoformerTS\")\n",
    "\n",
    "def cargar_datos(ruta_csv):\n",
    "    df = pd.read_csv(ruta_csv, low_memory=False)\n",
    "    df[\"con_fecha\"]      = pd.to_datetime(df[\"con_fecha\"], errors=\"coerce\")\n",
    "    df[\"con_dateticket\"] = pd.to_datetime(df[\"con_dateticket\"], errors=\"coerce\")\n",
    "    df[\"fecha_embarque\"] = pd.to_datetime(df[\"fecha_embarque\"], errors=\"coerce\")\n",
    "    columnas = [\n",
    "        \"fecha_embarque\", \"tipo_agrupacion\",\n",
    "        \"dia_del_anio_sin\", \"dia_del_anio_cos\",\n",
    "        \"dia_embarque_sin\", \"dia_embarque_cos\",\n",
    "        \"hora_embarque_sin\", \"hora_embarque_cos\",\n",
    "        \"is_weekend\",\n",
    "        \"mes_embarque_sin\", \"mes_embarque_cos\",\n",
    "        \"week_of_year_sin\", \"week_of_year_cos\",\n",
    "        \"season_1\", \"season_2\", \"season_3\", \"season_4\",\n",
    "        \"is_festivo_nacional\", \"is_festivo_local\",\n",
    "        \"is_eid_aladha\", \"is_eid_aladha_prev\", \"is_eid_aladha_post\",\n",
    "        \"is_eid_alfitr\", \"is_eid_alfitr_prev\", \"is_eid_alfitr_post\",\n",
    "        \"is_mawlid_nabi\",\n",
    "        \"is_monday\", \"is_tuesday\", \"is_wednesday\", \"is_thursday\",\n",
    "        \"is_friday\", \"is_saturday\", \"is_sunday\",\n",
    "        \"weekday_sin\", \"weekday_cos\",\n",
    "    ]\n",
    "    df = df[columnas]\n",
    "    return df, df[df[\"tipo_agrupacion\"] == 1].copy(), df[df[\"tipo_agrupacion\"] == 0].copy()\n",
    "\n",
    "\n",
    "def procesar_dataset_completo(df_orig, df_filtrado, target):\n",
    "    rango = pd.date_range(df_orig[\"fecha_embarque\"].min(),\n",
    "                          df_orig[\"fecha_embarque\"].max(), freq=\"D\")\n",
    "    df_daily = (df_filtrado.groupby(\"fecha_embarque\").size()\n",
    "                .reindex(rango, fill_value=0)\n",
    "                .reset_index(name=target)\n",
    "                .rename(columns={\"index\": \"fecha_embarque\"}))\n",
    "\n",
    "    df_temp = (df_orig.drop(columns=[\"tipo_agrupacion\"])\n",
    "               .drop_duplicates(\"fecha_embarque\")\n",
    "               .set_index(\"fecha_embarque\")\n",
    "               .reindex(rango)\n",
    "               .reset_index()\n",
    "               .rename(columns={\"index\": \"fecha_embarque\"}))\n",
    "\n",
    "    cat_cols = [c for c in df_temp.columns\n",
    "                if c.startswith(\"is_\") or c.startswith(\"season_\")]\n",
    "    for c in cat_cols:\n",
    "        df_temp[c] = df_temp[c].ffill().fillna(0)\n",
    "\n",
    "    df_temp[\"dia_del_anio\"] = df_temp[\"fecha_embarque\"].dt.dayofyear\n",
    "    df_temp[\"dia_del_anio_sin\"] = np.sin(2*np.pi*df_temp[\"dia_del_anio\"]/365)\n",
    "    df_temp[\"dia_del_anio_cos\"] = np.cos(2*np.pi*df_temp[\"dia_del_anio\"]/365)\n",
    "\n",
    "    df_temp[\"dia_embarque\"] = df_temp[\"fecha_embarque\"].dt.day\n",
    "    df_temp[\"dia_embarque_sin\"] = np.sin(2*np.pi*df_temp[\"dia_embarque\"]/31)\n",
    "    df_temp[\"dia_embarque_cos\"] = np.cos(2*np.pi*df_temp[\"dia_embarque\"]/31)\n",
    "\n",
    "    df_temp[\"mes_embarque\"] = df_temp[\"fecha_embarque\"].dt.month\n",
    "    df_temp[\"mes_embarque_sin\"] = np.sin(2*np.pi*df_temp[\"mes_embarque\"]/12)\n",
    "    df_temp[\"mes_embarque_cos\"] = np.cos(2*np.pi*df_temp[\"mes_embarque\"]/12)\n",
    "\n",
    "    df_temp[\"week_of_year\"] = df_temp[\"fecha_embarque\"].dt.isocalendar().week\n",
    "    df_temp[\"week_of_year_sin\"] = np.sin(2*np.pi*df_temp[\"week_of_year\"]/52)\n",
    "    df_temp[\"week_of_year_cos\"] = np.cos(2*np.pi*df_temp[\"week_of_year\"]/52)\n",
    "\n",
    "    if df_temp[\"weekday_sin\"].isna().any() or df_temp[\"weekday_cos\"].isna().any():\n",
    "        df_temp[\"weekday\"] = df_temp[\"fecha_embarque\"].dt.weekday\n",
    "        df_temp[\"weekday_sin\"] = np.sin(2*np.pi*df_temp[\"weekday\"]/7)\n",
    "        df_temp[\"weekday_cos\"] = np.cos(2*np.pi*df_temp[\"weekday\"]/7)\n",
    "        df_temp = df_temp.drop(columns=[\"weekday\"])\n",
    "\n",
    "    df_temp = df_temp.drop(columns=[\"dia_del_anio\", \"dia_embarque\",\n",
    "                                    \"mes_embarque\", \"week_of_year\"])\n",
    "\n",
    "    df_final = (pd.merge(df_temp, df_daily,\n",
    "                         on=\"fecha_embarque\", how=\"left\")\n",
    "                  .sort_values(\"fecha_embarque\")\n",
    "                  .fillna({target: 0}))\n",
    "    df_final[target] = df_final[target].fillna(0)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_final[f\"{target}_norm\"] = scaler.fit_transform(df_final[[target]])\n",
    "    df_final = df_final.fillna(0)\n",
    "    return df_final, scaler\n",
    "\n",
    "\n",
    "def crear_secuencias(df, target_col, lookback=7):\n",
    "    feats = df.columns.difference([\"fecha_embarque\", target_col, f\"{target_col}_norm\"])\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(df)):\n",
    "        X.append(df.iloc[i-lookback:i][feats].values.astype(\"float32\"))\n",
    "        y.append(df.iloc[i][f\"{target_col}_norm\"])\n",
    "    return np.array(X, dtype=\"float32\"), np.array(y, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def split_temporal(X, y, ratios=(0.7, 0.15, 0.15)):\n",
    "    p1 = int(len(X)*ratios[0]); p2 = p1 + int(len(X)*ratios[1])\n",
    "    return (X[:p1], y[:p1]), (X[p1:p2], y[p1:p2]), (X[p2:], y[p2:])\n",
    "\n",
    "class SeriesDecomposition(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=7): super().__init__(); self.k = kernel_size\n",
    "    def build(self, s):\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.pool = AveragePooling1D(self.k, 1, \"same\")\n",
    "    def call(self,x):\n",
    "        x_n = self.norm(x); trend = self.pool(x_n); return trend, x_n - trend\n",
    "\n",
    "\n",
    "class AutoCorrelationMechanism(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads): super().__init__(); self.d, self.h = d_model, heads\n",
    "    def build(self, s):\n",
    "        self.att  = MultiHeadAttention(self.h, key_dim=self.d // self.h)\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "    def call(self,x):\n",
    "        x_n = self.norm(x); return self.att(x_n, x_n, x_n)\n",
    "\n",
    "\n",
    "class AutoformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads, d_ff=128, drop=0.1):\n",
    "        super().__init__(); self.d, self.h = d_model, heads\n",
    "        self.ff = tf.keras.Sequential([Dense(d_ff, activation=\"relu\"),\n",
    "                                       Dropout(drop), Dense(d_model)])\n",
    "        self.drop = Dropout(drop)\n",
    "    def build(self,s):\n",
    "        self.autocorr = AutoCorrelationMechanism(self.d, self.h)\n",
    "        self.dec1 = SeriesDecomposition(); self.dec2 = SeriesDecomposition()\n",
    "    def call(self,x,training=True):\n",
    "        x1 = x + self.drop(self.autocorr(x), training=training)\n",
    "        t1, s1 = self.dec1(x1)\n",
    "        x2 = s1 + self.drop(self.ff(s1, training=training), training=training)\n",
    "        t2, s2 = self.dec2(x2)\n",
    "        return t1 + t2, s2\n",
    "\n",
    "def crear_autoformer_optimizado(seq_len,n_feat,d_model=64,heads=4,\n",
    "                                blocks=2,drop=0.1,lr=5e-4):\n",
    "    inp = Input(shape=(seq_len, n_feat))\n",
    "    x   = BatchNormalization()(Conv1D(d_model, 1, padding=\"same\")(inp))\n",
    "    t, s = SeriesDecomposition()(x); trend = [t]\n",
    "    for _ in range(blocks):\n",
    "        t_new, s = AutoformerBlock(d_model, heads, d_model*2, drop)(s)\n",
    "        t = t + t_new; trend.append(t)\n",
    "    x = BatchNormalization()(Conv1D(d_model, 1, padding=\"same\")(Concatenate()(trend)))\n",
    "    x = Add()([x, s])\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(drop)(Dense(d_model, activation=\"relu\", kernel_regularizer=l2(1e-4))(x))\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer=Adam(lr, clipnorm=0.5, epsilon=1e-7),\n",
    "                  loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "def entrenar(model,Xtr,ytr,Xval,yval,epochs=30,pat=10):\n",
    "    cb = [EarlyStopping(monitor=\"val_loss\",\n",
    "                        patience=pat,\n",
    "                        restore_best_weights=True,\n",
    "                        verbose=0),\n",
    "          ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                            factor=0.5,\n",
    "                            patience=pat // 2,\n",
    "                            min_lr=1e-6,\n",
    "                            verbose=0)]\n",
    "    model.fit(Xtr, ytr,\n",
    "              validation_data=(Xval, yval),\n",
    "              epochs=epochs, batch_size=32,\n",
    "              callbacks=cb, verbose=0)\n",
    "\n",
    "def evaluar(model,Xval,yval,scaler):\n",
    "    y_pred = model.predict(Xval, verbose=0).flatten()\n",
    "    r2 = r2_score(yval, y_pred)\n",
    "    rmse_n = sqrt(mean_squared_error(yval, y_pred))\n",
    "    mae_n = mean_absolute_error(yval, y_pred)\n",
    "    mape_n = np.mean(np.abs((yval - y_pred) / np.maximum(np.abs(yval), 1e-8))) * 100\n",
    "\n",
    "    y_pred_o = scaler.inverse_transform(y_pred.reshape(-1,1)).flatten()\n",
    "    y_o      = scaler.inverse_transform(yval.reshape(-1,1)).flatten()\n",
    "    rmse = sqrt(mean_squared_error(y_o, y_pred_o))\n",
    "    mae  = mean_absolute_error(y_o, y_pred_o)\n",
    "    mape = np.mean(np.abs((y_o - y_pred_o) / np.maximum(np.abs(y_o), 1e-8))) * 100\n",
    "\n",
    "    return {\"R2\": r2, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\n",
    "            \"RMSE_norm\": rmse_n, \"MAE_norm\": mae_n, \"MAPE_norm\": mape_n}\n",
    "\n",
    "def run_once(ruta_csv,lookback,d_mod,heads,drop,lr,epochs=30):\n",
    "    df, df_pas, _ = cargar_datos(ruta_csv)\n",
    "    df_proc, scaler = procesar_dataset_completo(df, df_pas, \"total_pasajeros\")\n",
    "    X, y = crear_secuencias(df_proc, \"total_pasajeros\", lookback)\n",
    "    (Xtr,ytr),(Xval,yval),_ = split_temporal(X,y)\n",
    "    model = crear_autoformer_optimizado(lookback, X.shape[2], d_mod,\n",
    "                                        heads, 2, drop, lr)\n",
    "    entrenar(model, Xtr, ytr, Xval, yval, epochs=epochs)\n",
    "    metrics = evaluar(model, Xval, yval, scaler)\n",
    "    tf.keras.backend.clear_session()\n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_csv = (\"/content/drive/MyDrive/3 - Master Computacion y \"\n",
    "                \"Sistemas Inteligentes/PFM/Datasets/dataset_final_convariablesdedias.csv\")\n",
    "\n",
    "    grid = {\"look\":[5,7,10],\n",
    "            \"dmod\":[32,64],\n",
    "            \"head\":[2,4],\n",
    "            \"drop\":[0.1,0.2],\n",
    "            \"lr\":[1e-3,5e-4]}\n",
    "    combos = [c for c in itertools.product(*grid.values())\n",
    "              if c[1] % c[2] == 0]    \n",
    "\n",
    "    logger.info(\"Calculando metrica base (7‑64‑4‑0.1‑1e‑3)…\")\n",
    "    base = run_once(ruta_csv, 7, 64, 4, 0.1, 1e-3, epochs=30)\n",
    "    R2_BASE = base[\"R2\"]\n",
    "    logger.info(f\"   R² base = {R2_BASE:.4f}\")\n",
    "\n",
    "    resultados = []; t0 = time.time()\n",
    "    for look,dmod,heads,drop,lr in combos:\n",
    "        logger.info(f\"HPO  look={look} d_model={dmod} heads={heads} \"\n",
    "                    f\"drop={drop} lr={lr}\")\n",
    "        met = run_once(ruta_csv, look, dmod, heads, drop, lr, epochs=30)\n",
    "        resultados.append({\n",
    "            \"lookback\": look, \"d_model\": dmod, \"heads\": heads,\n",
    "            \"dropout\": drop, \"lr\": lr,\n",
    "            \"R2\":   met[\"R2\"],\n",
    "            \"RMSE\": met[\"RMSE\"],\n",
    "            \"MAE\":  met[\"MAE\"],\n",
    "            \"MAPE\": met[\"MAPE\"],\n",
    "            \"Mejora_R2(%)\": 100 * (met[\"R2\"] - R2_BASE) / abs(R2_BASE)\n",
    "        })\n",
    "\n",
    "    df_hpo = (pd.DataFrame(resultados)\n",
    "                .sort_values(\"R2\", ascending=False))\n",
    "    df_hpo.to_csv(\"resultados_hpo_autoformer.csv\", index=False)\n",
    "\n",
    "    logger.info(f\"HPO completado en {time.time()-t0:.1f}s → \"\n",
    "                f\"resultados_hpo_autoformer.csv\")\n",
    "    print(\"\\nTOP 5 combinaciones ordenadas por R²\\n\")\n",
    "    print(df_hpo.head(5).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM3sWx6KaQP5m3CsfAAMQU1",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
